{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Poet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TextLibrary class: text library for training, encoding, batch generation,\n",
    "# and formatted source display\n",
    "class TextLibrary:\n",
    "    def __init__(self, filenames, max=100000000):\n",
    "        self.filenames = filenames\n",
    "        self.data=''\n",
    "        self.files=[]\n",
    "        index = 1\n",
    "        for filename in filenames:\n",
    "            fd={}\n",
    "            fd[\"name\"] = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.c2i = {}\n",
    "            self.i2c = {}\n",
    "            try:\n",
    "                f = open(filename)\n",
    "                dat = f.read(max)\n",
    "                self.data += dat\n",
    "                fd[\"data\"] = dat\n",
    "                fd[\"index\"] = index\n",
    "                index += 1\n",
    "                self.files.append(fd)\n",
    "                f.close()\n",
    "            except OSError:\n",
    "                print(\"  ERROR: Cannot read: \", filename)\n",
    "        ind = 0\n",
    "        for c in self.data: # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "            \n",
    "    def print_colored_IPython(self, textlist, pre='', post=''):\n",
    "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n','<br>')\n",
    "            if ind==0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind%16]+\";\\\">\" + txt +\\\n",
    "                       \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "        \n",
    "    def source_highlight(self, txt, minQuoteSize=10):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc=[(\"Sources: \", 0)]\n",
    "        sc=False\n",
    "        noquote = ''\n",
    "        while len(tx)>0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1>mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f[\"name\"]\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote)>0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ],mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN,mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote)>0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.printColoredIPython(out)\n",
    "        if len(qts)>0:  # print references, if there is at least one source\n",
    "            self.printColoredIPython(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "    \n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rewind = True\n",
    "        else:\n",
    "            rewind = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rewind\n",
    "    \n",
    "    def decode(self, ar):\n",
    "         return ''.join([self.i2c[ic] for ic in ar])\n",
    "            \n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0,len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "    \n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]],dtype=int)\n",
    "        return ar\n",
    "        \n",
    "    def get_sample(self, length):\n",
    "        s, rewind = self.get_slice(length+1)\n",
    "        X = np.array([self.c2i[c] for c in s[:-1]],dtype=int)\n",
    "        y = np.array([self.c2i[c] for c in s[1:]],dtype=int)\n",
    "        return (X, y, rewind)\n",
    "    \n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = np.array([self.c2i[c] for c in s[:-1]],dtype=int)\n",
    "        y = np.array([self.c2i[c] for c in s[1:]],dtype=int)\n",
    "        return (X, y)\n",
    "    \n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = np.zeros((batch_size,length),dtype=int)\n",
    "        smpy = np.zeros((batch_size,length),dtype=int)\n",
    "        for i in range(batch_size):\n",
    "            smpX[i,:], smpy[i,:], _ = self.get_sample(length)\n",
    "        return smpX, smpy\n",
    "        \n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = np.zeros((batch_size,length),dtype=int)\n",
    "        smpy = np.zeros((batch_size,length),dtype=int)\n",
    "        for i in range(batch_size):\n",
    "            smpX[i,:], smpy[i,:] = self.get_random_sample(length)\n",
    "        return smpX, smpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters and data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"TinyShakespeare\",\n",
    "    \"description\": \"Small Shakespeare 'standard' corpus\",\n",
    "    \"lib\": [\n",
    "        'data/tiny-shakespeare.txt',\n",
    "    ]\n",
    "}\n",
    "\n",
    "textlib = TextLibrary(libdesc[\"lib\"])\n",
    "\n",
    "# Model parameter:\n",
    "model_params_shakespeare = {\n",
    "    \"model_name\": \"shakespeare\",\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 256,\n",
    "    \"layers\": 3,\n",
    "    \"learning_rate\": 1.e-3,\n",
    "    \"steps\": 80,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "\n",
    "# Look for optional json description of a library:\n",
    "if os.path.exists('bk/lib-phil-deen.json'):\n",
    "    with open('bk/lib-phil-deen.json') as data_file:    \n",
    "        libdescphil = json.load(data_file)\n",
    "        textlib = TextLibrary(libdescphil[\"lib\"])\n",
    "        model_params_phil = {\n",
    "            \"model_name\": \"phil\",\n",
    "            \"vocab_size\": len(textlib.i2c),\n",
    "            \"neurons\": 256,\n",
    "            \"layers\": 8,\n",
    "            \"learning_rate\": 1.e-3,\n",
    "            \"steps\": 128,\n",
    "            \"batch_size\": 80\n",
    "        }\n",
    "        model_params = model_params_phil\n",
    "else:\n",
    "    model_params = model_params_shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(p, dim):\n",
    "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
    "    for y in range(p.shape[0]):\n",
    "        for x in range(p.shape[1]):\n",
    "            o[y,x,p[y,x]]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = model_params['batch_size']\n",
    "vocab_size = model_params['vocab_size']\n",
    "steps = model_params['steps']\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if not use_cuda:\n",
    "    dtype = torch.FloatTensor\n",
    "else:\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def get_data():\n",
    "    X, y=textlib.get_random_sample_batch(batch_size, steps)\n",
    "    Xo = one_hot(X, vocab_size)\n",
    "    \n",
    "    Xt = Variable(dtype(torch.from_numpy(np.array(Xo,dtype=np.float32)).type(dtype)), requires_grad=False)\n",
    "    yt = Variable(torch.LongTensor(torch.from_numpy(y)), requires_grad=False)\n",
    "    if use_cuda:\n",
    "        yt = yt.cuda()\n",
    "    return Xt, yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The char-rnn model (deep LSTMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(Poet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.demb = nn.Linear(hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).type(dtype))\n",
    "        self.c0 = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size).type(dtype))\n",
    "\n",
    "    def forward(self, input, steps, h0=None, c0=None):\n",
    "        if h0==None:\n",
    "            h0=self.h0\n",
    "        if c0==None:\n",
    "            c0=self.c0\n",
    "        hn, (self.h0, self.c0) = self.lstm(input, (h0, c0))\n",
    "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
    "        op = self.demb(hnr)\n",
    "        opr = op.view(-1, steps ,self.output_size)\n",
    "        return opr\n",
    "\n",
    "    def generate(self, n, start=None):\n",
    "        s=''\n",
    "        if start==None or len(start)==0:\n",
    "            start=' '\n",
    "        self.init_hidden(1)\n",
    "        for c in start:\n",
    "            X=np.array([[textlib.c2i[c]]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Variable(dtype(torch.from_numpy(np.array(Xo,dtype=np.float32)).type(dtype)), requires_grad=False)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "            # _, m=torch.max(yp,1)\n",
    "            # ic=m[0,0].data[0]\n",
    "        for i in range(n):\n",
    "            ypc=yp.data.cpu()\n",
    "            y_pred=ypc.numpy()\n",
    "            inds=list(range(self.output_size))\n",
    "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
    "            s=s+textlib.i2c[ind]\n",
    "            X=np.array([[ind]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Variable(dtype(torch.from_numpy(np.array(Xo,dtype=np.float32)).type(dtype)), requires_grad=False)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        return s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a poet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poet = Poet(vocab_size, model_params['neurons'], model_params['layers'], vocab_size)\n",
    "if use_cuda:\n",
    "    poet = poet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = model_params['learning_rate']\n",
    "\n",
    "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);\n",
    "\n",
    "bok=0\n",
    "\n",
    "def train(Xt, yt, bPr=False):\n",
    "    poet.zero_grad()\n",
    "\n",
    "    poet.init_hidden(Xt.size(0))\n",
    "    output = poet(Xt, steps)\n",
    "    \n",
    "    olin=output.view(-1,vocab_size)\n",
    "    _, ytp=torch.max(olin,1)\n",
    "    ytlin=yt.view(-1)\n",
    "\n",
    "    pr=0.0\n",
    "    if bPr: # Calculate precision\n",
    "        ok=0\n",
    "        nok=0\n",
    "        for i in range(ytlin.size()[0]):\n",
    "            i1=ytlin[i].data[0]\n",
    "            i2=ytp[i].data[0]\n",
    "            if i1==i2:\n",
    "                ok = ok + 1\n",
    "            else:\n",
    "                nok = nok+1\n",
    "            pr=ok/(ok+nok)\n",
    "            \n",
    "    loss = criterion(olin, ytlin)\n",
    "    ls = loss.data[0]\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "\n",
    "    return ls, pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls=0\n",
    "nrls=0\n",
    "if use_cuda:\n",
    "    intv=2500\n",
    "else:\n",
    "    intv=100\n",
    "for e in range(200000):\n",
    "    Xt, yt = get_data()\n",
    "    if (e+1)%intv==0:\n",
    "        l,pr=train(Xt,yt,True)\n",
    "    else:\n",
    "        l,pr=train(Xt,yt,False)        \n",
    "    ls=ls+l\n",
    "    nrls=nrls+1\n",
    "    if (e+1)%intv==0:\n",
    "        print(\"Loss:\",ls/nrls,\" Precision:\", pr)\n",
    "        nrls=0\n",
    "        ls=0\n",
    "        print(poet.generate(200,\"\\n\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
