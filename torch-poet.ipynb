{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Poet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TextLibrary class: text library for training, encoding, batch generation,\n",
    "# and formatted source display\n",
    "class TextLibrary:\n",
    "    def __init__(self, filenames, max=100000000):\n",
    "        self.filenames = filenames\n",
    "        self.data=''\n",
    "        self.files=[]\n",
    "        index = 1\n",
    "        for filename in filenames:\n",
    "            fd={}\n",
    "            fd[\"name\"] = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.c2i = {}\n",
    "            self.i2c = {}\n",
    "            try:\n",
    "                f = open(filename)\n",
    "                dat = f.read(max)\n",
    "                self.data += dat\n",
    "                fd[\"data\"] = dat\n",
    "                fd[\"index\"] = index\n",
    "                index += 1\n",
    "                self.files.append(fd)\n",
    "                f.close()\n",
    "            except OSError:\n",
    "                print(\"  ERROR: Cannot read: \", filename)\n",
    "        ind = 0\n",
    "        for c in self.data: # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "            \n",
    "    def print_colored_IPython(self, textlist, pre='', post=''):\n",
    "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n','<br>')\n",
    "            if ind==0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind%16]+\";\\\">\" + txt +\\\n",
    "                       \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "        \n",
    "    def source_highlight(self, txt, minQuoteSize=10):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc=[(\"Sources: \", 0)]\n",
    "        sc=False\n",
    "        noquote = ''\n",
    "        while len(tx)>0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1>mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f[\"name\"]\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote)>0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ],mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN,mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote)>0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.print_colored_IPython(out)\n",
    "        if len(qts)>0:  # print references, if there is at least one source\n",
    "            self.print_colored_IPython(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "    \n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rewind = True\n",
    "        else:\n",
    "            rewind = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rewind\n",
    "    \n",
    "    def decode(self, ar):\n",
    "         return ''.join([self.i2c[ic] for ic in ar])\n",
    "            \n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0,len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "    \n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]],dtype=int)\n",
    "        return ar\n",
    "        \n",
    "    def get_sample(self, length):\n",
    "        s, rewind = self.get_slice(length+1)\n",
    "        X = np.array([self.c2i[c] for c in s[:-1]],dtype=int)\n",
    "        y = np.array([self.c2i[c] for c in s[1:]],dtype=int)\n",
    "        return (X, y, rewind)\n",
    "    \n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = np.array([self.c2i[c] for c in s[:-1]],dtype=int)\n",
    "        y = np.array([self.c2i[c] for c in s[1:]],dtype=int)\n",
    "        return (X, y)\n",
    "    \n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = np.zeros((batch_size,length),dtype=int)\n",
    "        smpy = np.zeros((batch_size,length),dtype=int)\n",
    "        for i in range(batch_size):\n",
    "            smpX[i,:], smpy[i,:], _ = self.get_sample(length)\n",
    "        return smpX, smpy\n",
    "        \n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = np.zeros((batch_size,length),dtype=int)\n",
    "        smpy = np.zeros((batch_size,length),dtype=int)\n",
    "        for i in range(batch_size):\n",
    "            smpX[i,:], smpy[i,:] = self.get_random_sample(length)\n",
    "        return smpX, smpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters and data sources\n",
    "\n",
    "The library description can contain a list of text-files. It is possible, to add files in different languages. The net will learn by itself to generate text in only one of the languages. Color-markup is used in generated texts to identify memorized parts of the original texts.\n",
    "``` python\n",
    "libdesc = {\n",
    "    \"name\": \"My lib\",\n",
    "    \"description\": \"several texts\",\n",
    "    \"lib\": [\n",
    "        'data/some-english.txt',\n",
    "        'data/some-french.txt',\n",
    "        'data/some-german.txt',\n",
    "        'data/some-more.txt'\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"TinyShakespeare\",\n",
    "    \"description\": \"Small Shakespeare 'standard' corpus\",\n",
    "    \"lib\": [\n",
    "        'data/tiny-shakespeare.txt',\n",
    "    ]\n",
    "}\n",
    "\n",
    "textlib = TextLibrary(libdesc[\"lib\"])\n",
    "\n",
    "use_shakespeare = False\n",
    "\n",
    "if use_shakespeare or not os.path.exists('data/lib.json'):\n",
    "    # Default model parameters for shakespeare:\n",
    "    model_params_shakespeare = {\n",
    "        \"model_name\": \"lib\",\n",
    "        \"vocab_size\": len(textlib.i2c),\n",
    "        \"neurons\": 256,\n",
    "        \"layers\": 2,\n",
    "        \"learning_rate\": 1.e-3,\n",
    "        \"steps\": 80,\n",
    "        \"batch_size\": 128\n",
    "    }\n",
    "    model_params = model_params_shakespeare\n",
    "else:        \n",
    "    # Look for optional json description of a library:\n",
    "    with open('data/lib.json') as data_file:    \n",
    "        libdesc = json.load(data_file)\n",
    "        textlib = TextLibrary(libdesc[\"lib\"])\n",
    "        model_params_lib = {\n",
    "            \"model_name\": \"lib\",\n",
    "            \"vocab_size\": len(textlib.i2c),\n",
    "            \"neurons\": 512,\n",
    "            \"layers\": 4,\n",
    "            \"learning_rate\": 2.e-4,\n",
    "            \"steps\": 80,\n",
    "            \"batch_size\": 128\n",
    "        }\n",
    "        model_params = model_params_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(p, dim):\n",
    "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
    "    for y in range(p.shape[0]):\n",
    "        for x in range(p.shape[1]):\n",
    "            o[y,x,p[y,x]]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = model_params['batch_size']\n",
    "vocab_size = model_params['vocab_size']\n",
    "steps = model_params['steps']\n",
    "\n",
    "force_cpu=False\n",
    "\n",
    "if torch.cuda.is_available() and force_cpu is not True:\n",
    "    device='cuda'\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device='cpu'\n",
    "    use_cuda = False\n",
    "\n",
    "def get_data():\n",
    "    X, y=textlib.get_random_sample_batch(batch_size, steps)\n",
    "    Xo = one_hot(X, vocab_size)\n",
    "    \n",
    "    # Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32)), requires_grad=False, dtype=torch.float32, device=device)\n",
    "    # yt = Tensor(torch.from_numpy(y), requires_grad=False, dtype=torch.int32, device=device)\n",
    "    Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(device)\n",
    "    Xt.requires_grad_(False)\n",
    "    yt = torch.LongTensor(torch.from_numpy(np.array(y,dtype=np.int64))).to(device)\n",
    "    yt.requires_grad_(False)\n",
    "    return Xt, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gpu_mem(context=\"all\"):\n",
    "    if use_cuda:\n",
    "        print(\"[{}] Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(context,torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The char-rnn model (deep LSTMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Poet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
    "        super(Poet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.device=device\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
    "        \n",
    "        self.demb = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputx, steps):\n",
    "        self.lstm.flatten_parameters()\n",
    "        hn, (self.h0, self.c0) = self.lstm(inputx.to(self.device), (self.h0, self.c0))\n",
    "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
    "        op = self.demb(hnr)\n",
    "        opr = op.view(-1, steps ,self.output_size)\n",
    "        return opr\n",
    "\n",
    "    def generate(self, n, start=None):\n",
    "        s=''\n",
    "        torch.set_grad_enabled(False)\n",
    "        if start==None or len(start)==0:\n",
    "            start=' '\n",
    "        self.init_hidden(1)\n",
    "        for c in start:\n",
    "            X=np.array([[textlib.c2i[c]]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        for i in range(n):\n",
    "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
    "            y_pred=ypc.numpy()\n",
    "            inds=list(range(self.output_size))\n",
    "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
    "            s=s+textlib.i2c[ind]\n",
    "            X=np.array([[ind]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        torch.set_grad_enabled(True)\n",
    "        return s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a poet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poet = Poet(vocab_size, model_params['neurons'], model_params['layers'], vocab_size, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = model_params['learning_rate']\n",
    "\n",
    "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);\n",
    "\n",
    "bok=0\n",
    "\n",
    "def train(Xt, yt, bPr=False):\n",
    "    poet.zero_grad()\n",
    "\n",
    "    poet.init_hidden(Xt.size(0))\n",
    "    output = poet(Xt, steps)\n",
    "    \n",
    "    olin=output.view(-1,vocab_size)\n",
    "    _, ytp=torch.max(olin,1)\n",
    "    ytlin=yt.view(-1)\n",
    "\n",
    "    pr=0.0\n",
    "    if bPr: # Calculate precision\n",
    "        ok=0\n",
    "        nok=0\n",
    "        for i in range(ytlin.size()[0]):\n",
    "            i1=ytlin[i].item()\n",
    "            i2=ytp[i].item()\n",
    "            if i1==i2:\n",
    "                ok = ok + 1\n",
    "            else:\n",
    "                nok = nok+1\n",
    "            pr=ok/(ok+nok)\n",
    "            \n",
    "    loss = criterion(olin, ytlin)\n",
    "    ls = loss.item()\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "\n",
    "    return ls, pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls=0\n",
    "nrls=0\n",
    "if use_cuda:\n",
    "    intv=250\n",
    "else:\n",
    "    intv=10\n",
    "for e in range(2500000):\n",
    "    Xt, yt = get_data()\n",
    "    if (e+1)%intv==0:\n",
    "        l,pr=train(Xt,yt,True)\n",
    "    else:\n",
    "        l,pr=train(Xt,yt,False)        \n",
    "    ls=ls+l\n",
    "    nrls=nrls+1\n",
    "    if (e+1)%intv==0:\n",
    "        print(\"Loss: {} Precision: {}\".format(ls/nrls, pr))\n",
    "        # if use_cuda:\n",
    "        #    print(\"Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n",
    "        nrls=0\n",
    "        ls=0\n",
    "        tgen=poet.generate(500,\"\\n\\n\")\n",
    "        textlib.source_highlight(tgen,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n",
    "    \n",
    "tgen=poet.generate(1000,\"\\n\\n\")\n",
    "detectPlagiarism(tgen, textlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "def doDialog():\n",
    "    # temperature = 0.6  # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    \n",
    "    print(\"Please enter some dialog.\")\n",
    "    print(\"The net will answer according to your input.\")\n",
    "    print(\"'bye' for end,\")\n",
    "    print(\"'reset' to reset the conversation context,\")\n",
    "    # print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "    print(\"    to change character of the dialog.\")\n",
    "    # print(\"    Current temperature={}.\".format(temperature))\n",
    "    print()\n",
    "    xso = None\n",
    "    bye = False\n",
    "        \n",
    "    while not bye:\n",
    "        print(\"> \", end=\"\")\n",
    "        prompt = input()\n",
    "        if prompt == 'bye':\n",
    "            bye = True\n",
    "            print(\"Good bye!\")\n",
    "            continue\n",
    "        tgen=poet.generate(1000,prompt)\n",
    "        # print(xso.replace(\"\\\\n\",\"\\n\"))\n",
    "        textlib.source_highlight(tgen, 10)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "best_prec1=64.4\n",
    "\n",
    "save_checkpoint({\n",
    "            'epoch': e,\n",
    "            'arch': \"poet8\",\n",
    "            'state_dict': poet.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : opti.state_dict(),\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
