{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-poet/blob/master/torch_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28i44jSzUlon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen  # Py3\n",
    "except:\n",
    "    print(\"This notebook requires Python 3.\")\n",
    "try:\n",
    "    import pathlib\n",
    "except:\n",
    "    print(\"At least python 3.5 is needed.\")\n",
    "    \n",
    "try: # Colab instance?\n",
    "    from google.colab import drive\n",
    "except: # Not? ignore.\n",
    "    pass\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eE-jm072kOKv"
   },
   "source": [
    "# 0. System configuration\n",
    "\n",
    "This notebook can either run on a local jupyter server, or on google cloud.\n",
    "If a GPU is available, it will be used for training (if `force_cpu` is not set to `True`).\n",
    "\n",
    "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
    "\n",
    "Similarily, the text corpora that are used for training, can be cached on drive or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVyGr-BCiJlR"
   },
   "outputs": [],
   "source": [
    "# force_cpu=True: use CPU for training, even if a GPU is available.\n",
    "#    Note: inference uses CPU always, because that is faster.\n",
    "force_cpu=False\n",
    "\n",
    "# Define where snapshots of training data are stored:\n",
    "colab_google_drive_snapshots=True\n",
    "\n",
    "# Define if training data (the texts downloaded from internet) are cached:\n",
    "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
    "local_jupyter_data_cache=True       # In local jupyter mode cache to local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ply0tFmz4O1E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 1.4.0, running on GPU\n"
     ]
    }
   ],
   "source": [
    "is_colab_notebook = 'google.colab' in sys.modules\n",
    "torch_version = torch.__version__\n",
    "\n",
    "if torch.cuda.is_available() and force_cpu is not True:\n",
    "    device='cuda'\n",
    "    use_cuda = True\n",
    "    print(f\"PyTorch {torch_version}, running on GPU\")\n",
    "    if is_colab_notebook:\n",
    "        card = !nvidia-smi\n",
    "        if len(card)>=8:\n",
    "            try:\n",
    "                gpu_type=card[7][6:25]\n",
    "                gpu_memory=card[8][33:54]\n",
    "                print(f\"Colab GPU: {gpu_type}, GPU Memory: {gpu_memory}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "else:\n",
    "    device='cpu'\n",
    "    use_cuda = False\n",
    "    print(f\"{torch_version}, running on CPU\")\n",
    "    if colab_notebook:\n",
    "        print(\"Note: on Google Colab, make sure to select:\")\n",
    "        print(\"      Runtime / Change Runtime Type / Hardware accelerator: GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTRmAT1f4O1I"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots:\n",
    "        mountpoint='/content/drive'\n",
    "        root_path='/content/drive/My Drive'\n",
    "        if not os.path.exists(root_path):\n",
    "            drive.mount(mountpoint)\n",
    "        if not os.path.exists(root_path):\n",
    "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
    "            colab_google_drive_snapshots=False\n",
    "    else:\n",
    "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
    "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
    "else:\n",
    "    root_path='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTRmAT1f4O1I"
   },
   "outputs": [],
   "source": [
    "def one_hot(p, dim):\n",
    "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
    "    for y in range(p.shape[0]):\n",
    "        for x in range(p.shape[1]):\n",
    "            o[y,x,p[y,x]]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoutSg5IUlot"
   },
   "source": [
    "# 1. Text data collection\n",
    "\n",
    "## 1.1 Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts of a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzXrUVe-4O1N"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pMoAJ-SVDm2"
   },
   "outputs": [],
   "source": [
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.cache_dir=text_data_cache_directory\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        index = 1\n",
    "        for descriptor, author, title in descriptors:\n",
    "            fd = {}\n",
    "            cache_name=self.get_cache_name(author, title)\n",
    "            if os.path.exists(cache_name):\n",
    "                is_cached=True\n",
    "            else:\n",
    "                is_cached=False\n",
    "            valid=False\n",
    "            if descriptor[:4] == 'http' and is_cached is False:\n",
    "                try:\n",
    "                    print(f\"Downloading {cache_name}\")\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    dat=dat.replace('\\r', '')  # get rid of pesky LFs \n",
    "                    self.data += dat\n",
    "                    fd[\"title\"] = title\n",
    "                    fd[\"author\"] = author\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    valid=True\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"title\"] = title\n",
    "                fd[\"author\"] = author\n",
    "                try:\n",
    "                    if is_cached is True:\n",
    "                        print(f\"Reading {cache_name} from cache\")\n",
    "                        f = open(cache_name)\n",
    "                    else:    \n",
    "                        f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                    valid=True\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
    "                try:\n",
    "                    print(f\"Caching {cache_name}\")\n",
    "                    f = open(cache_name, 'w')\n",
    "                    f.write(dat)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
    "                \n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "\n",
    "    def get_cache_name(self, author, title):\n",
    "        if self.cache_dir is None:\n",
    "            return None\n",
    "        cname=f\"{author} - {title}.txt\"\n",
    "        cache_filepath=os.path.join(self.cache_dir , cname)\n",
    "        return cache_filepath\n",
    "        \n",
    "    def display_colored_html(self, textlist, dark_mode=False, display_ref_anchor=True, pre='', post=''):\n",
    "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
    "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
    "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
    "        if dark_mode is False:\n",
    "            bgcolors=bgcolorsWht\n",
    "        else:\n",
    "            bgcolors=bgcolorsDrk\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                if display_ref_anchor is True:\n",
    "                    anchor=\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "                else:\n",
    "                    anchor=\"\"\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+ anchor\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False, display_ref_anchor=True):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f\"{f['author']}: {f['title']}\"\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out, dark_mode=dark_mode, display_ref_anchor=display_ref_anchor)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, dark_mode=dark_mode, display_ref_anchor=display_ref_anchor, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return np.array(smpX), np.array(smpy)\n",
    "    \n",
    "    def get_random_onehot_sample_batch(self, batch_size, length):\n",
    "        X, y = self.get_random_sample_batch(batch_size, length)\n",
    "        return one_hot(X,len(self.i2c)), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcMGC5KDUloz"
   },
   "source": [
    "## 1.2 Data sources\n",
    "\n",
    "Data sources can either be files from local filesystem, or for colab notebooks from google drive, or http(s) links.\n",
    "\n",
    "The `name` given will be use as directory name for both snapshots and model data caches.\n",
    "\n",
    "Each entry in the `lib` array contains of:\n",
    "\n",
    "1. a local filename or https(s) link,\n",
    "2. an Author's name\n",
    "3. a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmUwv47UVA7r"
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"Women-Writers\",\n",
    "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
    "    \"lib\": [\n",
    "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),   # local file example\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', 'Virginia Wolf', 'Voyage out'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', 'Jane Austen', 'Emma')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "else:\n",
    "    if local_jupyter_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "\n",
    "if data_cache_path is not None:\n",
    "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(data_cache_path):\n",
    "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
    "    else:\n",
    "        with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
    "            json.dump(libdesc,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmUwv47UVA7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Women-Writers/Data/Jane Austen - Pride and Prejudice.txt from cache\n",
      "Reading ./Women-Writers/Data/Emily Brontë - Wuthering Heights.txt from cache\n",
      "Reading ./Women-Writers/Data/Virginia Wolf - Voyage out.txt from cache\n",
      "Reading ./Women-Writers/Data/Jane Austen - Emma.txt from cache\n"
     ]
    }
   ],
   "source": [
    "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zP0Hcs82lWYI"
   },
   "source": [
    "# 2. The deep LSTM model\n",
    "\n",
    "# 2.1 Model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEM7Y3GxUlo0"
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"model_name\": libdesc['name'],\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 256,\n",
    "    \"layers\": 2,\n",
    "    \"learning_rate\": 1.e-3,\n",
    "    \"steps\": 80,\n",
    "    \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_JWmbgPUlpG"
   },
   "source": [
    "## 2.2 The char-rnn model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcsP8OYlUlpH"
   },
   "outputs": [],
   "source": [
    "class Poet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
    "        super(Poet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.device=device\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
    "        \n",
    "        self.demb = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputx, steps):\n",
    "        self.lstm.flatten_parameters()\n",
    "        hn, (self.h0, self.c0) = self.lstm(inputx.to(self.device), (self.h0, self.c0))\n",
    "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
    "        op = self.demb(hnr)\n",
    "        opr = op.view(-1, steps ,self.output_size)\n",
    "        return opr\n",
    "\n",
    "    def generate(self, n, start=None):\n",
    "        s=''\n",
    "        torch.set_grad_enabled(False)\n",
    "        if start==None or len(start)==0:\n",
    "            start=' '\n",
    "        self.init_hidden(1)\n",
    "        for c in start:\n",
    "            X=np.array([[textlib.c2i[c]]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        for i in range(n):\n",
    "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
    "            y_pred=ypc.numpy()\n",
    "            inds=list(range(self.output_size))\n",
    "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
    "            s=s+textlib.i2c[ind]\n",
    "            X=np.array([[ind]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        torch.set_grad_enabled(True)\n",
    "        return s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymdU_wIWUlpK"
   },
   "source": [
    "## 2.3 Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7WuQ142UlpL"
   },
   "outputs": [],
   "source": [
    "poet = Poet(model_params['vocab_size'], model_params['neurons'], model_params['layers'], model_params['vocab_size'], device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJp6Sjw6UlpP"
   },
   "source": [
    "## 2.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeFxMxyuUlpQ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = model_params['learning_rate']\n",
    "\n",
    "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtKdvxNymXpM"
   },
   "source": [
    "## 2.5 Helper Functions\n",
    "\n",
    "These allow to save or restore the training data. Saving and restoring can either be performed:\n",
    "\n",
    "* Jupyter: store/restore in a local directory,\n",
    "* Colab: store/restore on google drive. The training-code (using load_checkpoint()) will display an authentication url and code input-box in order to be able to access your google drive from this notebook. This allows to continue training sessions (or inference) after the Colab session was terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots is True:\n",
    "        snapshot_path=os.path.join(root_path,f\"Colab Notebooks/{model_params['model_name']}/Snapshots\")\n",
    "    else:\n",
    "        snapshot_path=None\n",
    "else:\n",
    "    snapshot_path=os.path.join(root_path,f\"{model_params['model_name']}/Snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "def get_project_path():\n",
    "    if snapshot_path is None:\n",
    "        return None\n",
    "    project_path_ext=f\"model-{model_params['vocab_size']}x{model_params['steps']}x{model_params['layers']}x{model_params['neurons']}\"\n",
    "    return os.path.join(snapshot_path, project_path_ext)\n",
    "\n",
    "def create_project_path():\n",
    "    if snapshot_path is None:\n",
    "        return None\n",
    "    ppath=get_project_path()\n",
    "    pathlib.Path(ppath).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "if snapshot_path is not None:\n",
    "    pathlib.Path(snapshot_path).mkdir(parents=True, exist_ok=True)\n",
    "    create_project_path()\n",
    "    with open(os.path.join(get_project_path(),'model_params.json'),'w') as f:\n",
    "        json.dump(model_params,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "best_pr=0.0\n",
    "\n",
    "def save_checkpoint(epoch, loss, pr, filename='checkpoint.pth.tar'):\n",
    "    if snapshot_path is None:\n",
    "        return\n",
    "    global best_pr\n",
    "    state={\n",
    "            'epoch': epoch,\n",
    "            'model_config': model_params,\n",
    "            'state_dict': poet.state_dict(),\n",
    "            'optimizer' : opti.state_dict(),\n",
    "            'precision': pr,\n",
    "            'loss': loss,\n",
    "        }\n",
    "    project_path=get_project_path()\n",
    "    save_file=os.path.join(project_path,filename)\n",
    "    best_file=os.path.join(project_path,'model_best.pth.tar')\n",
    "    torch.save(state, save_file)\n",
    "    if pr>best_pr:\n",
    "        best_pr=pr\n",
    "        shutil.copyfile(save_file, best_file )\n",
    "        print(f\"Saved best precision model, prec={pr}\")\n",
    "    else:\n",
    "        print(f\"saved last model data, prec={pr}\")\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    if snapshot_path is None:\n",
    "        return 0,0\n",
    "    project_path=get_project_path()\n",
    "    load_file=os.path.join(project_path,filename)\n",
    "    if not os.path.exists(load_file):\n",
    "        print(load_file)\n",
    "        print(\"No saved state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    state=torch.load(load_file)\n",
    "    mod_conf = state['model_config']\n",
    "    if (mod_conf['model_name']!=model_params['model_name']):\n",
    "        print(f\"Warning: project has been renamed from {mod_conf['model_name']} to {model_param['model_name']}\")\n",
    "        mod_conf['model_name']=model_params['model_name']\n",
    "    if model_params!=mod_conf:\n",
    "        print(f\"The saved model has a different configuration than the current model: {mod_conf} vs. {model_params}\")\n",
    "        print(\"Cannot restore state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    poet.load_state_dict(state['state_dict'])\n",
    "    opti.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    loss = state['loss']\n",
    "    best_pr = state['precision']\n",
    "    print(f\"Continuing from saved state epoch={epoch}, loss={loss}\")  # Save is not necessarily on epoch boundary, so that's approx.\n",
    "    return epoch,loss\n",
    "\n",
    "# def one_hot(p, dim):\n",
    "#     o=np.zeros(p.shape+(dim,), dtype=int32)\n",
    "#     for y in range(p.shape[0]):\n",
    "#         for x in range(p.shape[1]):\n",
    "#             o[y,x,p[y,x]]=1\n",
    "#     return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_-ISha4nqhy"
   },
   "source": [
    "# 3. Training\n",
    "\n",
    "If there is already saved training data, this step is optional, and alternatively, ch. 4 can be continued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KDRpEm0n7B-"
   },
   "source": [
    "## 3.1 Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pZlsZ1Pnm5Z"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    Xo, y=textlib.get_random_onehot_sample_batch(model_params['batch_size'], model_params['steps'])\n",
    "    # Xo = one_hot(X, model_params['vocab_size'])\n",
    "    \n",
    "    # Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32)), requires_grad=False, dtype=torch.float32, device=device)\n",
    "    # yt = Tensor(torch.from_numpy(y), requires_grad=False, dtype=torch.int32, device=device)\n",
    "    Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(device)\n",
    "    Xt.requires_grad_(False)\n",
    "    yt = torch.LongTensor(torch.from_numpy(np.array(y,dtype=np.int64))).to(device)\n",
    "    yt.requires_grad_(False)\n",
    "    return Xt, yt\n",
    "\n",
    "def train(Xt, yt, bPr=False):\n",
    "    poet.zero_grad()\n",
    "\n",
    "    poet.init_hidden(Xt.size(0))\n",
    "    output = poet(Xt, model_params['steps'])\n",
    "    \n",
    "    olin=output.view(-1,model_params['vocab_size'])\n",
    "    _, ytp=torch.max(olin,1)\n",
    "    ytlin=yt.view(-1)\n",
    "\n",
    "    pr=0.0\n",
    "    if bPr: # Calculate precision\n",
    "        ok=0\n",
    "        nok=0\n",
    "        for i in range(ytlin.size()[0]):\n",
    "            i1=ytlin[i].item()\n",
    "            i2=ytp[i].item()\n",
    "            if i1==i2:\n",
    "                ok = ok + 1\n",
    "            else:\n",
    "                nok = nok+1\n",
    "            pr=ok/(ok+nok)\n",
    "            \n",
    "    loss = criterion(olin, ytlin)\n",
    "    ls = loss.item()\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "\n",
    "    return ls, pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC36hNRKUlpU"
   },
   "source": [
    "## 3.2 The actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9-3GUQ4UlpV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing from saved state epoch=8249, loss=1.313832278251648\n",
      "Epoch 8250 Loss: 1.3381035327911377 Precision: 0.59482421875\n",
      "Saved best precision model, prec=0.59482421875\n",
      "Memory allocated: 18663424 max_alloc: 195193856 cached: 211812352 max_cached: 211812352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\"Miss Women!<br><span style=\"background-color:#ebdef0;\"><br><br>Heathcliff </span>of the  he waved--it's too odd, if music, claim earth.<br><br>'An-to resire I'd hear sect, I must,' she depen<span style=\"background-color:#eadbd8;\">sed her hu</span><span style=\"background-color:#e2d7d5;\">ld from the</span> s<span style=\"background-color:#e2d7d5;\">on of extreme</span><span style=\"background-color:#e2d7d5;\">ly almost </span>cold-den! I am knowed<br>to ecefficably elent lorg<span style=\"background-color:#eadbd8;\">ly, Helen, </span><span style=\"background-color:#eadbd8;\">though, of</span><span style=\"background-color:#eadbd8;\"> course and </span><span style=\"background-color:#d8daef;\">accordance </span>places as any tix<span style=\"background-color:#eadbd8;\">ing a whole </span><span style=\"background-color:#eadbd8;\">of themselves</span> n<span style=\"background-color:#eadbd8;\">even, and </span>survete attacken who spicely moder, so<span style=\"background-color:#eadbd8;\">ings<br>were </span>to<span style=\"background-color:#e2d7d5;\"> so very su</span>ch again him.<br>To tI may consel<span style=\"background-color:#eadbd8;\">itations o</span><span style=\"background-color:#d8daef;\">n the most </span><span style=\"background-color:#ebdef0;\">break, and </span>its reas<span style=\"background-color:#ebdef0;\">on his brea</span>kfast now whom<span style=\"background-color:#e2d7d5;\"> so entirely co</span>mplality "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8500 Loss: 1.3109075045585632 Precision: 0.6017578125\n",
      "Saved best precision model, prec=0.6017578125\n",
      "Memory allocated: 18663424 max_alloc: 195193856 cached: 211812352 max_cached: 211812352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'The manc<span style=\"background-color:#eadbd8;\">e say it is</span> a madely mak<span style=\"background-color:#eadbd8;\">e over; but</span>, you chure really. But<span style=\"background-color:#d8daef;\"> defend them</span>?' and her keep:<br>dreadful<br>father,<br><span style=\"background-color:#eadbd8;\">to what you s</span>uppoint,<span style=\"background-color:#e2d7d5;\"> only weak,</span><span style=\"background-color:#ebdef0;\">' he said </span>Mrs<span style=\"background-color:#d8daef;\">. Bingley in </span><span style=\"background-color:#ebdef0;\">the whole b</span><span style=\"background-color:#ebdef0;\">rought into </span><span style=\"background-color:#eadbd8;\">there can be</span>gin away. It perva<span style=\"background-color:#eadbd8;\">te when she </span><span style=\"background-color:#eadbd8;\">beginning. It</span> was a<span style=\"background-color:#ebdef0;\">t since hi</span>m<span style=\"background-color:#ebdef0;\"> for nothing in </span>melay.<br>Appr<span style=\"background-color:#eadbd8;\">ight while we</span>ll on<br>advising cornemn them<span style=\"background-color:#d8daef;\">.<br><br>It is very </span>lives--a<span style=\"background-color:#eadbd8;\">t the most </span>is ha<span style=\"background-color:#eadbd8;\">d wandered </span>her.<br>One was<br>attacl<span style=\"background-color:#e2d7d5;\">e is not. </span>I'm no<span style=\"background-color:#eadbd8;\"> going. He </span><span style=\"background-color:#e2d7d5;\">leave her. </span>B<span style=\"background-color:#eadbd8;\">rightly, a</span><span style=\"background-color:#ebdef0;\">s a pleasure </span>he sa<span style=\"background-color:#eadbd8;\">t and very </span>ordowal<span style=\"background-color:#eadbd8;\">s. When he </span>tol"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8750 Loss: 1.3043513827323914 Precision: 0.603515625\n",
      "Saved best precision model, prec=0.603515625\n",
      "Memory allocated: 18663424 max_alloc: 195193856 cached: 211812352 max_cached: 211812352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "     The order his easy deve<span style=\"background-color:#ebdef0;\">nding his </span>vouching<span style=\"background-color:#ebdef0;\">, or more </span>of<span style=\"background-color:#ebdef0;\"> them; but </span>I as<span style=\"background-color:#e2d7d5;\">sure; and </span>the unacistic<br>strain apart<span style=\"background-color:#d8daef;\">s. She is </span>cousin. Bitter me. [_Lev<span style=\"background-color:#eadbd8;\">ing and now </span>consi<span style=\"background-color:#d8daef;\">ment from the </span>arms<span style=\"background-color:#ebdef0;\"> death!' and </span>she's the<br>cray hati<span style=\"background-color:#e2d7d5;\">ture?\"<br><br>\"I </span><span style=\"background-color:#d8daef;\">never deserve </span>me<span style=\"background-color:#e2d7d5;\"> think herself </span>had so, 'w<span style=\"background-color:#e2d7d5;\">hat is as </span>world i<span style=\"background-color:#e2d7d5;\">s to want </span>us,<span style=\"background-color:#e2d7d5;\"> who is not</span>hing formic<span style=\"background-color:#e2d7d5;\">ation was r</span>ught write<br>class on s<span style=\"background-color:#eadbd8;\">o<br>little s</span>ighby<span style=\"background-color:#ebdef0;\"> the speak</span>ing I really, a provide<span style=\"background-color:#ebdef0;\"> raising his </span><span style=\"background-color:#d8daef;\">books, and </span><span style=\"background-color:#ebdef0;\">it was a co</span>untry's<span style=\"background-color:#eadbd8;\"> back at her </span><span style=\"background-color:#ebdef0;\">in the table-</span>t<span style=\"background-color:#eadbd8;\">rees that </span><span style=\"background-color:#eadbd8;\">were something </span>invent<br>flate"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000 Loss: 1.302339304447174 Precision: 0.6044921875\n",
      "Saved best precision model, prec=0.6044921875\n",
      "Memory allocated: 18663424 max_alloc: 195193856 cached: 211812352 max_cached: 211812352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Brsoh<span style=\"background-color:#eadbd8;\">ed Rachel, wh</span>ile<span style=\"background-color:#eadbd8;\"> then all </span><span style=\"background-color:#e2d7d5;\">ever conceived</span>.<span style=\"background-color:#eadbd8;\"><br><br>\"Oh, the</span>re'<span style=\"background-color:#e2d7d5;\">s, Harriet</span>'s<span style=\"background-color:#e2d7d5;\"> have had be</span>en<span style=\"background-color:#e2d7d5;\"> inquiries.</span> [_Execution by<span style=\"background-color:#e2d7d5;\"> hers.\"<br><br>S</span><span style=\"background-color:#ebdef0;\">he took<br>and </span>horrid<span style=\"background-color:#eadbd8;\"> check her</span>self, mealted.  Frank<span style=\"background-color:#eadbd8;\"> leaves, and the</span><span style=\"background-color:#e2d7d5;\">n love with her, th</span>ough ghes<span style=\"background-color:#ebdef0;\">s<br>for the </span>gre<span style=\"background-color:#e2d7d5;\">at nonsense </span><span style=\"background-color:#eadbd8;\">and forced him</span>, but<br>curio<span style=\"background-color:#ebdef0;\">sion that </span><span style=\"background-color:#eadbd8;\">the thought th</span><span style=\"background-color:#e2d7d5;\">e question to a</span><span style=\"background-color:#e2d7d5;\"> little distr</span><span style=\"background-color:#d8daef;\">ibution of </span>hit<span style=\"background-color:#eadbd8;\"> was<br>something </span>nec<span style=\"background-color:#ebdef0;\">essing, and</span>, n<span style=\"background-color:#eadbd8;\">or individual</span> likely<br><span style=\"background-color:#eadbd8;\">out in her </span><span style=\"background-color:#e2d7d5;\">particulars, which </span><span style=\"background-color:#eadbd8;\">was the gr</span>osp<span style=\"background-color:#ebdef0;\">er to<br>the </span><span style=\"background-color:#eadbd8;\">careless. H</span><span style=\"background-color:#d8daef;\">is father's<br></span>hotsables.  Th<span style=\"background-color:#e2d7d5;\">e offering t</span><span style=\"background-color:#e2d7d5;\">ried with a </span>sti"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9250 Loss: 1.2960497784614562 Precision: 0.5953125\n",
      "saved last model data, prec=0.5953125\n",
      "Memory allocated: 18663424 max_alloc: 195193856 cached: 211812352 max_cached: 211812352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'Chy<span style=\"background-color:#e2d7d5;\"> girl!\" said </span><span style=\"background-color:#eadbd8;\">Rachel's hand.</span>\"\"<br><br>\"I am_ ver<span style=\"background-color:#eadbd8;\">y more of </span><span style=\"background-color:#d8daef;\">you can ha</span><span style=\"background-color:#e2d7d5;\">d been ever </span>can be ten<span style=\"background-color:#e2d7d5;\">ue, my dear</span> good<br>no<span style=\"background-color:#e2d7d5;\">tes with a </span>bargon<span style=\"background-color:#ebdef0;\">able about </span><span style=\"background-color:#eadbd8;\">them. But the</span> chur<span style=\"background-color:#eadbd8;\">e of doubt. </span>She pr<span style=\"background-color:#eadbd8;\">ecided to </span>_very_<span style=\"background-color:#e2d7d5;\"> not have<br>co</span>ver<span style=\"background-color:#e2d7d5;\">ed as to the </span><span style=\"background-color:#d8daef;\">Project Gutenberg-</span><span style=\"background-color:#d8daef;\">information i</span>n<span style=\"background-color:#eadbd8;\"> some such </span>as yon<span style=\"background-color:#eadbd8;\">'s<br>something</span><span style=\"background-color:#eadbd8;\"><br>about his</span><br><span style=\"background-color:#ebdef0;\">age, and s</span>tranger)<span style=\"background-color:#e2d7d5;\">--but you </span>c<span style=\"background-color:#ebdef0;\">an reason </span><span style=\"background-color:#ebdef0;\">obliged to g</span>et<span style=\"background-color:#d8daef;\"> up to her </span><span style=\"background-color:#ebdef0;\">uncle to<br>be</span><br>news, hau<span style=\"background-color:#d8daef;\">nted with </span>conneply<span style=\"background-color:#e2d7d5;\"> of her nat</span>ive; es<span style=\"background-color:#e2d7d5;\"> her amiable </span><span style=\"background-color:#ebdef0;\">exclaimed.  'A</span>nd<span style=\"background-color:#e2d7d5;\"> is<br>never </span>tiy<span style=\"background-color:#ebdef0;\">e I might </span>be very<br>edsel<span style=\"background-color:#e2d7d5;\">ted for the a</span>imphemark. Let "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9500 Loss: 1.2917531037330627 Precision: 0.6138671875\n",
      "Saved best precision model, prec=0.6138671875\n",
      "Memory allocated: 18663424 max_alloc: 195193856 cached: 211812352 max_cached: 211812352\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'W<span style=\"background-color:#eadbd8;\">e could me</span>-avage<span style=\"background-color:#ebdef0;\">: it is no</span> be<span style=\"background-color:#e2d7d5;\">stood, and </span><span style=\"background-color:#e2d7d5;\">I could mea</span>l<span style=\"background-color:#ebdef0;\"> me!' I re</span>flec<span style=\"background-color:#eadbd8;\">t,\" she as</span>ked.<br><br>'Shank<span style=\"background-color:#ebdef0;\"> for making </span>my mind in sor<span style=\"background-color:#ebdef0;\">t what I c</span><span style=\"background-color:#d8daef;\">ould desire</span><span style=\"background-color:#ebdef0;\"><br>they had </span>d<span style=\"background-color:#ebdef0;\">everting t</span><span style=\"background-color:#ebdef0;\">he<br>property </span>of<br>errand-rate.\"<br><br>\"Luckus<span style=\"background-color:#eadbd8;\">e done, the</span><br>glooured<span style=\"background-color:#e2d7d5;\"> must, and </span><span style=\"background-color:#e2d7d5;\">you are be</span>en b<span style=\"background-color:#e2d7d5;\">y too hand</span><span style=\"background-color:#ebdef0;\">, I have not </span>veng<span style=\"background-color:#ebdef0;\">ing happiness</span><span style=\"background-color:#e2d7d5;\">: and the </span>twenty-pluckled now, of any likely love to kept into pity<span style=\"background-color:#d8daef;\"> as you<br>di</span>stressed.  '<span style=\"background-color:#eadbd8;\">It seemed to </span>Lint<span style=\"background-color:#eadbd8;\">on, being </span><span style=\"background-color:#eadbd8;\">any one in </span>epposling Franking!\" and<br>if say,<br>obtain i<span style=\"background-color:#e2d7d5;\">f his love</span> _my_<br>cuntails which _-_Mis<span style=\"background-color:#d8daef;\">s_ BINGLEY _</span>inf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls=0\n",
    "nrls=0\n",
    "if use_cuda:\n",
    "    intv=250\n",
    "else:\n",
    "    intv=10\n",
    "\n",
    "create_project_path()\n",
    "epoch_start, _ = load_checkpoint()\n",
    "\n",
    "for e in range(epoch_start,2500000):\n",
    "    Xt, yt = get_data()\n",
    "    if (e+1)%intv==0:\n",
    "        l,pr=train(Xt,yt,True)\n",
    "    else:\n",
    "        l,pr=train(Xt,yt,False)        \n",
    "    ls=ls+l\n",
    "    nrls=nrls+1\n",
    "    if (e+1)%intv==0:\n",
    "        print(\"Epoch {} Loss: {} Precision: {}\".format(e+1,ls/nrls, pr))\n",
    "        save_checkpoint(e,ls/nrls,pr)\n",
    "        if use_cuda:\n",
    "            print(\"Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n",
    "        nrls=0\n",
    "        ls=0\n",
    "        tgen=poet.generate(500,\"\\n\\n\")\n",
    "        textlib.source_highlight(tgen,minQuoteSize=10,dark_mode=use_dark_mode,display_ref_anchor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dsSPjPsUlpY"
   },
   "source": [
    "# 4. Text generation\n",
    "\n",
    "## 4.1 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdpCtjvfUlpZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(generatedtext, minQuoteSize=minQuoteLength,dark_mode=use_dark_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjCd8CHLUlpd"
   },
   "source": [
    "## 4.2 Dialog with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfiL1_64Ulpe"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "def doDialog():\n",
    "    # temperature = 0.6  # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    \n",
    "    print(\"Please enter some dialog.\")\n",
    "    print(\"The net will answer according to your input.\")\n",
    "    print(\"'bye' for end,\")\n",
    "    print(\"'reset' to reset the conversation context,\")\n",
    "    # print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "    print(\"    to change character of the dialog.\")\n",
    "    # print(\"    Current temperature={}.\".format(temperature))\n",
    "    print()\n",
    "    xso = None\n",
    "    bye = False\n",
    "        \n",
    "    while not bye:\n",
    "        print(\"> \", end=\"\")\n",
    "        prompt = input()\n",
    "        if prompt == 'bye':\n",
    "            bye = True\n",
    "            print(\"Good bye!\")\n",
    "            continue\n",
    "        tgen=poet.generate(1000,prompt)\n",
    "        # print(xso.replace(\"\\\\n\",\"\\n\"))\n",
    "        textlib.source_highlight(tgen, minQuoteSize=10,dark_mode=use_dark_mode,display_ref_anchor=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-vHDcHupruq"
   },
   "outputs": [],
   "source": [
    "load_checkpoint(filename=\"model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-vHDcHupruq"
   },
   "outputs": [],
   "source": [
    "print(\"Sample text:\")\n",
    "print(\"\")\n",
    "tgen=poet.generate(1000,\"\\n\\n\")\n",
    "detectPlagiarism(tgen, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mGGsuFRUlpi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spZ0rUZJm92c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of torch_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
