{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "torch_poet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/torch-poet/blob/master/torch_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28i44jSzUlon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from urllib.request import urlopen  # Py3\n",
        "\n",
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoutSg5IUlot",
        "colab_type": "text"
      },
      "source": [
        "## Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pMoAJ-SVDm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor, name in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = name\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = name\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return np.array(smpX), np.array(smpy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcMGC5KDUloz",
        "colab_type": "text"
      },
      "source": [
        "## Model parameters and data sources\n",
        "\n",
        "`TextLibrary` class: text library for training, encoding, batch generation,\n",
        "and formatted source display. It read some books from Project Gutenberg\n",
        "and supports creation of training batches. The output functions support\n",
        "highlighting to allow to compare generated texts with the actual sources\n",
        "to help to identify identical (memorized) parts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmUwv47UVA7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Woman Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', \"Shakespeare: Collected Works\"\n",
        "        #  Project Gutenberg: Pride and Prejudice_ by Jane Austen, Wuthering Heights by Emily Brontë, The Voyage Out by Virginia Woolf and Emma_by Jane Austen\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', \"Jane Austen: Pride and Prejudice\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', \"Emily Brontë: Wuthering Heights\"),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', \"Virginia Wolf: Voyage out\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', \"Jane Austen: Emma\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEM7Y3GxUlo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_params = {\n",
        "    \"model_name\": \"lib\",\n",
        "    \"vocab_size\": len(textlib.i2c),\n",
        "    \"neurons\": 768,\n",
        "    \"layers\": 6,\n",
        "    \"learning_rate\": 1.e-3,\n",
        "    \"steps\": 80,\n",
        "    \"batch_size\": 128\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CD7869jUlo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(p, dim):\n",
        "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
        "    for y in range(p.shape[0]):\n",
        "        for x in range(p.shape[1]):\n",
        "            o[y,x,p[y,x]]=1\n",
        "    return o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Ya3QVPUlo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = model_params['batch_size']\n",
        "vocab_size = model_params['vocab_size']\n",
        "steps = model_params['steps']\n",
        "\n",
        "force_cpu=False\n",
        "\n",
        "if torch.cuda.is_available() and force_cpu is not True:\n",
        "    device='cuda'\n",
        "    use_cuda = True\n",
        "    card = !nvidia-smi\n",
        "    print(\"Running on GPU\")\n",
        "    if len(card)>=8:\n",
        "        try:\n",
        "            gpu_type=card[7][6:25]\n",
        "            gpu_memory=card[8][33:54]\n",
        "            print(f\"GPU: {gpu_type}, GPU Memory: {gpu_memory}\")\n",
        "        except Exception as e:\n",
        "            pass\n",
        "else:\n",
        "    device='cpu'\n",
        "    use_cuda = False\n",
        "    print(\"Running on CPU\")\n",
        "    print(\"Note: on Google Colab, make sure to select:\")\n",
        "    print(\"      Runtime / Change Runtime Type / Hardware accelerator: GPU\")\n",
        "\n",
        "def get_data():\n",
        "    X, y=textlib.get_random_sample_batch(batch_size, steps)\n",
        "    Xo = one_hot(X, vocab_size)\n",
        "    \n",
        "    # Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32)), requires_grad=False, dtype=torch.float32, device=device)\n",
        "    # yt = Tensor(torch.from_numpy(y), requires_grad=False, dtype=torch.int32, device=device)\n",
        "    Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(device)\n",
        "    Xt.requires_grad_(False)\n",
        "    yt = torch.LongTensor(torch.from_numpy(np.array(y,dtype=np.int64))).to(device)\n",
        "    yt.requires_grad_(False)\n",
        "    return Xt, yt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV9U1z8dUlpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_gpu_mem(context=\"all\"):\n",
        "    if use_cuda:\n",
        "        print(\"[{}] Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(context,torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_JWmbgPUlpG",
        "colab_type": "text"
      },
      "source": [
        "## The char-rnn model (deep LSTMs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcsP8OYlUlpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Poet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
        "        super(Poet, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "        self.device=device\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
        "        \n",
        "        self.demb = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
        "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "    def forward(self, inputx, steps):\n",
        "        self.lstm.flatten_parameters()\n",
        "        hn, (self.h0, self.c0) = self.lstm(inputx.to(self.device), (self.h0, self.c0))\n",
        "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
        "        op = self.demb(hnr)\n",
        "        opr = op.view(-1, steps ,self.output_size)\n",
        "        return opr\n",
        "\n",
        "    def generate(self, n, start=None):\n",
        "        s=''\n",
        "        torch.set_grad_enabled(False)\n",
        "        if start==None or len(start)==0:\n",
        "            start=' '\n",
        "        self.init_hidden(1)\n",
        "        for c in start:\n",
        "            X=np.array([[textlib.c2i[c]]])\n",
        "            Xo=one_hot(X,self.output_size)\n",
        "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
        "            ypl = self.forward(Xt,1)\n",
        "            ypl2 = ypl.view(-1,self.output_size)\n",
        "            yp = self.softmax(ypl2)\n",
        "        for i in range(n):\n",
        "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
        "            y_pred=ypc.numpy()\n",
        "            inds=list(range(self.output_size))\n",
        "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
        "            s=s+textlib.i2c[ind]\n",
        "            X=np.array([[ind]])\n",
        "            Xo=one_hot(X,self.output_size)\n",
        "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
        "            ypl = self.forward(Xt,1)\n",
        "            ypl2 = ypl.view(-1,self.output_size)\n",
        "            yp = self.softmax(ypl2)\n",
        "        torch.set_grad_enabled(True)\n",
        "        return s    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymdU_wIWUlpK",
        "colab_type": "text"
      },
      "source": [
        "## Create a poet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7WuQ142UlpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poet = Poet(vocab_size, model_params['neurons'], model_params['layers'], vocab_size, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJp6Sjw6UlpP",
        "colab_type": "text"
      },
      "source": [
        "## Training helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeFxMxyuUlpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = model_params['learning_rate']\n",
        "\n",
        "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);\n",
        "\n",
        "bok=0\n",
        "\n",
        "def train(Xt, yt, bPr=False):\n",
        "    poet.zero_grad()\n",
        "\n",
        "    poet.init_hidden(Xt.size(0))\n",
        "    output = poet(Xt, steps)\n",
        "    \n",
        "    olin=output.view(-1,vocab_size)\n",
        "    _, ytp=torch.max(olin,1)\n",
        "    ytlin=yt.view(-1)\n",
        "\n",
        "    pr=0.0\n",
        "    if bPr: # Calculate precision\n",
        "        ok=0\n",
        "        nok=0\n",
        "        for i in range(ytlin.size()[0]):\n",
        "            i1=ytlin[i].item()\n",
        "            i2=ytp[i].item()\n",
        "            if i1==i2:\n",
        "                ok = ok + 1\n",
        "            else:\n",
        "                nok = nok+1\n",
        "            pr=ok/(ok+nok)\n",
        "            \n",
        "    loss = criterion(olin, ytlin)\n",
        "    ls = loss.item()\n",
        "    loss.backward()\n",
        "    opti.step()\n",
        "\n",
        "    return ls, pr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC36hNRKUlpU",
        "colab_type": "text"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "q9-3GUQ4UlpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls=0\n",
        "nrls=0\n",
        "if use_cuda:\n",
        "    intv=250\n",
        "else:\n",
        "    intv=10\n",
        "for e in range(2500000):\n",
        "    Xt, yt = get_data()\n",
        "    if (e+1)%intv==0:\n",
        "        l,pr=train(Xt,yt,True)\n",
        "    else:\n",
        "        l,pr=train(Xt,yt,False)        \n",
        "    ls=ls+l\n",
        "    nrls=nrls+1\n",
        "    if (e+1)%intv==0:\n",
        "        print(\"Loss: {} Precision: {}\".format(ls/nrls, pr))\n",
        "        # if use_cuda:\n",
        "        #    print(\"Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n",
        "        nrls=0\n",
        "        ls=0\n",
        "        tgen=poet.generate(500,\"\\n\\n\")\n",
        "        textlib.source_highlight(tgen,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dsSPjPsUlpY",
        "colab_type": "text"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wdpCtjvfUlpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n",
        "    \n",
        "tgen=poet.generate(1000,\"\\n\\n\")\n",
        "detectPlagiarism(tgen, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjCd8CHLUlpd",
        "colab_type": "text"
      },
      "source": [
        "## Dialog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfiL1_64Ulpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "def doDialog():\n",
        "    # temperature = 0.6  # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    \n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    # print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    # print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "        \n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        tgen=poet.generate(1000,prompt)\n",
        "        # print(xso.replace(\"\\\\n\",\"\\n\"))\n",
        "        textlib.source_highlight(tgen, 10)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8mGGsuFRUlpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKEqzxcvUlpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "best_prec1=64.4\n",
        "\n",
        "save_checkpoint({\n",
        "            'epoch': e,\n",
        "            'arch': \"poet8\",\n",
        "            'state_dict': poet.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "            'optimizer' : opti.state_dict(),\n",
        "        })\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYEcftpVUlpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}