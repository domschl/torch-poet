{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of torch_poet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/torch-poet/blob/master/torch_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28i44jSzUlon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from urllib.request import urlopen  # Py3\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoutSg5IUlot",
        "colab_type": "text"
      },
      "source": [
        "## Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqYXy1m76u3E",
        "colab_type": "code",
        "outputId": "63ff368e-a11e-43b9-e430-25b51a1395fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pMoAJ-SVDm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor, name in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = name\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = name\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return np.array(smpX), np.array(smpy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcMGC5KDUloz",
        "colab_type": "text"
      },
      "source": [
        "## Model parameters and data sources\n",
        "\n",
        "`TextLibrary` class: text library for training, encoding, batch generation,\n",
        "and formatted source display. It read some books from Project Gutenberg\n",
        "and supports creation of training batches. The output functions support\n",
        "highlighting to allow to compare generated texts with the actual sources\n",
        "to help to identify identical (memorized) parts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmUwv47UVA7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Woman Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', \"Shakespeare: Collected Works\"\n",
        "        #  Project Gutenberg: Pride and Prejudice_ by Jane Austen, Wuthering Heights by Emily Brontë, The Voyage Out by Virginia Woolf and Emma_by Jane Austen\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', \"Jane Austen: Pride and Prejudice\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', \"Emily Brontë: Wuthering Heights\"),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', \"Virginia Wolf: Voyage out\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', \"Jane Austen: Emma\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEM7Y3GxUlo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_params = {\n",
        "    \"model_name\": \"lib\",\n",
        "    \"vocab_size\": len(textlib.i2c),\n",
        "    \"neurons\": 256,\n",
        "    \"layers\": 2,\n",
        "    \"learning_rate\": 1.e-3,\n",
        "    \"steps\": 80,\n",
        "    \"batch_size\": 128\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CD7869jUlo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(p, dim):\n",
        "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
        "    for y in range(p.shape[0]):\n",
        "        for x in range(p.shape[1]):\n",
        "            o[y,x,p[y,x]]=1\n",
        "    return o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Ya3QVPUlo8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ecc4b4c3-a5bc-4281-d742-052ce1a49108"
      },
      "source": [
        "batch_size = model_params['batch_size']\n",
        "vocab_size = model_params['vocab_size']\n",
        "steps = model_params['steps']\n",
        "\n",
        "force_cpu=False\n",
        "\n",
        "if torch.cuda.is_available() and force_cpu is not True:\n",
        "    device='cuda'\n",
        "    use_cuda = True\n",
        "    card = !nvidia-smi\n",
        "    print(\"Running on GPU\")\n",
        "    if len(card)>=8:\n",
        "        try:\n",
        "            gpu_type=card[7][6:25]\n",
        "            gpu_memory=card[8][33:54]\n",
        "            print(f\"GPU: {gpu_type}, GPU Memory: {gpu_memory}\")\n",
        "        except Exception as e:\n",
        "            pass\n",
        "else:\n",
        "    device='cpu'\n",
        "    use_cuda = False\n",
        "    print(\"Running on CPU\")\n",
        "    print(\"Note: on Google Colab, make sure to select:\")\n",
        "    print(\"      Runtime / Change Runtime Type / Hardware accelerator: GPU\")\n",
        "\n",
        "def get_data():\n",
        "    X, y=textlib.get_random_sample_batch(batch_size, steps)\n",
        "    Xo = one_hot(X, vocab_size)\n",
        "    \n",
        "    # Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32)), requires_grad=False, dtype=torch.float32, device=device)\n",
        "    # yt = Tensor(torch.from_numpy(y), requires_grad=False, dtype=torch.int32, device=device)\n",
        "    Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(device)\n",
        "    Xt.requires_grad_(False)\n",
        "    yt = torch.LongTensor(torch.from_numpy(np.array(y,dtype=np.int64))).to(device)\n",
        "    yt.requires_grad_(False)\n",
        "    return Xt, yt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on GPU\n",
            "GPU:  Tesla P4          , GPU Memory:      10MiB /  7611MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV9U1z8dUlpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_gpu_mem(context=\"all\"):\n",
        "    if use_cuda:\n",
        "        print(\"[{}] Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(context,torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_JWmbgPUlpG",
        "colab_type": "text"
      },
      "source": [
        "## The char-rnn model (deep LSTMs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcsP8OYlUlpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Poet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
        "        super(Poet, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size\n",
        "        self.device=device\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
        "        \n",
        "        self.demb = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
        "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "    def forward(self, inputx, steps):\n",
        "        self.lstm.flatten_parameters()\n",
        "        hn, (self.h0, self.c0) = self.lstm(inputx.to(self.device), (self.h0, self.c0))\n",
        "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
        "        op = self.demb(hnr)\n",
        "        opr = op.view(-1, steps ,self.output_size)\n",
        "        return opr\n",
        "\n",
        "    def generate(self, n, start=None):\n",
        "        s=''\n",
        "        torch.set_grad_enabled(False)\n",
        "        if start==None or len(start)==0:\n",
        "            start=' '\n",
        "        self.init_hidden(1)\n",
        "        for c in start:\n",
        "            X=np.array([[textlib.c2i[c]]])\n",
        "            Xo=one_hot(X,self.output_size)\n",
        "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
        "            ypl = self.forward(Xt,1)\n",
        "            ypl2 = ypl.view(-1,self.output_size)\n",
        "            yp = self.softmax(ypl2)\n",
        "        for i in range(n):\n",
        "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
        "            y_pred=ypc.numpy()\n",
        "            inds=list(range(self.output_size))\n",
        "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
        "            s=s+textlib.i2c[ind]\n",
        "            X=np.array([[ind]])\n",
        "            Xo=one_hot(X,self.output_size)\n",
        "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
        "            ypl = self.forward(Xt,1)\n",
        "            ypl2 = ypl.view(-1,self.output_size)\n",
        "            yp = self.softmax(ypl2)\n",
        "        torch.set_grad_enabled(True)\n",
        "        return s    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymdU_wIWUlpK",
        "colab_type": "text"
      },
      "source": [
        "## Create a poet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7WuQ142UlpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poet = Poet(vocab_size, model_params['neurons'], model_params['layers'], vocab_size, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJp6Sjw6UlpP",
        "colab_type": "text"
      },
      "source": [
        "## Training helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeFxMxyuUlpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = model_params['learning_rate']\n",
        "\n",
        "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfmdQ6zCMy2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5146faa3-eb6d-47f8-d70e-9608555ae60a"
      },
      "source": [
        "mountpoint='/content/drive'\n",
        "mountpath='/content/drive/My Drive'\n",
        "\n",
        "if not os.path.exists(mountpath):\n",
        "    drive.mount(mountpoint)\n",
        "\n",
        "best_pr=0.0\n",
        "\n",
        "def save_checkpoint(epoch, loss, pr, filename='checkpoint.pth.tar'):\n",
        "    global best_pr\n",
        "    state={\n",
        "            'epoch': epoch,\n",
        "            'model_config': model_params,\n",
        "            'state_dict': poet.state_dict(),\n",
        "            'optimizer' : opti.state_dict(),\n",
        "            'precision': pr,\n",
        "            'loss': loss,\n",
        "        }\n",
        "    save_path=os.path.join(mountpath,filename)\n",
        "    best_path=os.path.join(mountpath,'model_best.pth.tar')\n",
        "    torch.save(state, save_path)\n",
        "    if pr>best_pr:\n",
        "        best_pr=pr\n",
        "        shutil.copyfile(save_path, best_path )\n",
        "        print(f\"Saved best precision model, prec={pr}\")\n",
        "    else:\n",
        "        print(f\"saved last model data, prec={pr}\")\n",
        "\n",
        "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
        "    load_path=os.path.join(mountpath,filename)\n",
        "    if not os.path.exists(load_path):\n",
        "        print(\"No saved state, starting from scratch\")\n",
        "        return 0,0\n",
        "    state=torch.load(load_path)\n",
        "    mod_conf = state['model_config']\n",
        "    if model_params!=mod_conf:\n",
        "        print(f\"The saved model has a different configuration than the current model: {mod_conf} vs. {model_params}\")\n",
        "        print(\"Cannot restore state\")\n",
        "        return 0,0\n",
        "    poet.load_state_dict(state['state_dict'])\n",
        "    opti.load_state_dict(state['optimizer'])\n",
        "    epoch = state['epoch']\n",
        "    loss = state['loss']\n",
        "    print(f\"Continuing from saved state epoch={epoch}, loss={loss}\")  # Save is not necessarily on epoch boundary, so that's approx.\n",
        "    return epoch,loss\n",
        "\n",
        "\n",
        "def train(Xt, yt, bPr=False):\n",
        "    poet.zero_grad()\n",
        "\n",
        "    poet.init_hidden(Xt.size(0))\n",
        "    output = poet(Xt, steps)\n",
        "    \n",
        "    olin=output.view(-1,vocab_size)\n",
        "    _, ytp=torch.max(olin,1)\n",
        "    ytlin=yt.view(-1)\n",
        "\n",
        "    pr=0.0\n",
        "    if bPr: # Calculate precision\n",
        "        ok=0\n",
        "        nok=0\n",
        "        for i in range(ytlin.size()[0]):\n",
        "            i1=ytlin[i].item()\n",
        "            i2=ytp[i].item()\n",
        "            if i1==i2:\n",
        "                ok = ok + 1\n",
        "            else:\n",
        "                nok = nok+1\n",
        "            pr=ok/(ok+nok)\n",
        "            \n",
        "    loss = criterion(olin, ytlin)\n",
        "    ls = loss.item()\n",
        "    loss.backward()\n",
        "    opti.step()\n",
        "\n",
        "    return ls, pr"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC36hNRKUlpU",
        "colab_type": "text"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "q9-3GUQ4UlpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d899e155-3fe6-4e69-a26b-e63bfc9c147c"
      },
      "source": [
        "ls=0\n",
        "nrls=0\n",
        "if use_cuda:\n",
        "    intv=250\n",
        "else:\n",
        "    intv=10\n",
        "\n",
        "epoch_start, _ = load_checkpoint()\n",
        "\n",
        "for e in range(epoch_start,2500000):\n",
        "    Xt, yt = get_data()\n",
        "    if (e+1)%intv==0:\n",
        "        l,pr=train(Xt,yt,True)\n",
        "    else:\n",
        "        l,pr=train(Xt,yt,False)        \n",
        "    ls=ls+l\n",
        "    nrls=nrls+1\n",
        "    if (e+1)%intv==0:\n",
        "        print(\"Epoch {} Loss: {} Precision: {}\".format(e+1,ls/nrls, pr))\n",
        "        save_checkpoint(e,ls/nrls,pr)\n",
        "        if use_cuda:\n",
        "            print(\"Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n",
        "        nrls=0\n",
        "        ls=0\n",
        "        tgen=poet.generate(500,\"\\n\\n\")\n",
        "        textlib.source_highlight(tgen,10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing from saved state epoch=4499, loss=1.4508748860359193\n",
            "Epoch 4500 Loss: 1.402668833732605 Precision: 0.57880859375\n",
            "Saved best precision model, prec=0.57880859375\n",
            "Memory allocated: 18724864 max_alloc: 195419136 cached: 211812352 max_cached: 211812352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "illoc<span style=\"background-color:#eadbd8;\">ed that you're </span><sup>[3]</sup>liv<span style=\"background-color:#eadbd8;\">es thought</span><sup>[3]</sup>! [BING. BUNG.\r<br>\r<br>[_Prusemer<span style=\"background-color:#ebdef0;\"> short in </span><sup>[2]</sup>the\r<br>imin<span style=\"background-color:#ebdef0;\">isted that </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">the house, d</span><sup>[2]</sup>raw<span style=\"background-color:#d8daef;\">ing young </span><sup>[1]</sup>desir<span style=\"background-color:#eadbd8;\">ed it is to </span><sup>[3]</sup>many liush,<span style=\"background-color:#eadbd8;\"> and\r<br>were </span><sup>[3]</sup>soidation ha<span style=\"background-color:#ebdef0;\">sted in a </span><sup>[2]</sup>gladous thin earny, very\r<br>passs<span style=\"background-color:#e2d7d5;\">ile which s</span><sup>[4]</sup>uldered;\r<br>but har<span style=\"background-color:#e2d7d5;\">d\r<br>before t</span><sup>[4]</sup>he\r<br>complination curded,<span style=\"background-color:#ebdef0;\"> him,' he </span><sup>[2]</sup>looking longed; she\r<br>aggetauted\r<br>reper<span style=\"background-color:#eadbd8;\">ted to tell you</span><sup>[3]</sup>r travitial houring<span style=\"background-color:#ebdef0;\"> mine was </span><sup>[2]</sup>excu<span style=\"background-color:#eadbd8;\">sting in the </span><sup>[3]</sup>resomfthone of hoteb<span style=\"background-color:#e2d7d5;\">ution; and</span><sup>[4]</sup>\r<br>how us nor own man-lowment<span style=\"background-color:#ebdef0;\">ing inform</span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">ed; and you</span><sup>[2]</sup>r natterly.<span style=\"background-color:#eadbd8;\"> you thinking </span><sup>[3]</sup>toward or sweel "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4750 Loss: 1.432890214920044 Precision: 0.56865234375\n",
            "saved last model data, prec=0.56865234375\n",
            "Memory allocated: 18724864 max_alloc: 195419136 cached: 211812352 max_cached: 211812352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "you knowled any masten beswees in sio from fie<span style=\"background-color:#ebdef0;\">lding the </span><sup>[2]</sup>are assawhill, Susen ranisally exacment to tendially comsa. . . . Fo<span style=\"background-color:#e2d7d5;\">r might pr</span><sup>[4]</sup>ove at lawed Donved\r<br><span style=\"background-color:#d8daef;\">you, and I </span><sup>[1]</sup><span style=\"background-color:#e2d7d5;\">hope of mo</span><sup>[4]</sup>nli<span style=\"background-color:#ebdef0;\">chen, and\r<br></span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">she was\r<br>to</span><sup>[3]</sup> se<span style=\"background-color:#eadbd8;\">en,\" said\r<br></span><sup>[3]</sup>time<span style=\"background-color:#ebdef0;\"> and sighed, </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">recollection, </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">when he was </span><sup>[2]</sup>be<span style=\"background-color:#e2d7d5;\">en appeared t</span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">ogether, to </span><sup>[2]</sup>the peesoned anx to bresing blo<span style=\"background-color:#eadbd8;\">ad to leav</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">ing and cr</span><sup>[3]</sup>aing overr<span style=\"background-color:#d8daef;\">oom their </span><sup>[1]</sup>was sobe with me's lirst:<span style=\"background-color:#e2d7d5;\"> under par</span><sup>[4]</sup>i<span style=\"background-color:#ebdef0;\">t.  It was</span><sup>[2]</sup>\r<br>in<span style=\"background-color:#eadbd8;\">stribute the</span><sup>[3]</sup>ir with elentted by men the by anguited. \"You, \"What\r<br>th<span style=\"background-color:#e2d7d5;\">en some pa</span><sup>[4]</sup>ti<span style=\"background-color:#eadbd8;\">ed on the pr</span><sup>[3]</sup>os"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5000 Loss: 1.4162263512611388 Precision: 0.56806640625\n",
            "saved last model data, prec=0.56806640625\n",
            "Memory allocated: 18724864 max_alloc: 195419136 cached: 211812352 max_cached: 211812352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "are and a first anoteer of eyes behortably of eB<span style=\"background-color:#eadbd8;\">ook of her</span><sup>[3]</sup> fear<span style=\"background-color:#d8daef;\">iated in an</span><sup>[1]</sup>gar<span style=\"background-color:#e2d7d5;\">ious certain</span><sup>[4]</sup> medic<span style=\"background-color:#ebdef0;\">ation, if </span><sup>[2]</sup>he did<span style=\"background-color:#e2d7d5;\"> no saying </span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">in an even</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">sing his n</span><sup>[3]</sup>ews of along was awwor<span style=\"background-color:#eadbd8;\">ner in the </span><sup>[3]</sup>flateried\r<br>of bold to\r<br><span style=\"background-color:#eadbd8;\">deep water</span><sup>[3]</sup> Liz<span style=\"background-color:#ebdef0;\">ton is as </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">she was sta</span><sup>[2]</sup>ites what do<span style=\"background-color:#e2d7d5;\"> in her, a</span><sup>[4]</sup>nd best mi<span style=\"background-color:#eadbd8;\">stered in </span><sup>[3]</sup>her<span style=\"background-color:#eadbd8;\"> each, and </span><sup>[3]</sup>here abe clearation s<span style=\"background-color:#ebdef0;\">eturned the ple</span><sup>[2]</sup>asan<span style=\"background-color:#e2d7d5;\">ter general</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">.\"\r<br>\r<br>Emma sa</span><sup>[4]</sup>id remarkar<span style=\"background-color:#eadbd8;\">ate of her</span><sup>[3]</sup>\r<br>terriber<span style=\"background-color:#e2d7d5;\"> that good</span><sup>[4]</sup>, and unseads wrong blass, ly! She saum<span style=\"background-color:#eadbd8;\">ed,\" she t</span><sup>[3]</sup>wo<span style=\"background-color:#ebdef0;\"> with my comp</span><sup>[2]</sup>lai<span style=\"background-color:#e2d7d5;\">nt. Mrs. Weston </span><sup>[4]</sup>Apress<span style=\"background-color:#eadbd8;\">ice, the s</span><sup>[3]</sup>inging "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ba3b32bb4cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnrls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-c8076ea8fa4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(Xt, yt, bPr)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0molin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytlin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-uQpdiFRt1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hPZoXKUVDlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dsSPjPsUlpY",
        "colab_type": "text"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wdpCtjvfUlpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n",
        "    \n",
        "tgen=poet.generate(1000,\"\\n\\n\")\n",
        "detectPlagiarism(tgen, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjCd8CHLUlpd",
        "colab_type": "text"
      },
      "source": [
        "## Dialog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfiL1_64Ulpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "def doDialog():\n",
        "    # temperature = 0.6  # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    \n",
        "    print(\"Please enter some dialog.\")\n",
        "    print(\"The net will answer according to your input.\")\n",
        "    print(\"'bye' for end,\")\n",
        "    print(\"'reset' to reset the conversation context,\")\n",
        "    # print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "    print(\"    to change character of the dialog.\")\n",
        "    # print(\"    Current temperature={}.\".format(temperature))\n",
        "    print()\n",
        "    xso = None\n",
        "    bye = False\n",
        "        \n",
        "    while not bye:\n",
        "        print(\"> \", end=\"\")\n",
        "        prompt = input()\n",
        "        if prompt == 'bye':\n",
        "            bye = True\n",
        "            print(\"Good bye!\")\n",
        "            continue\n",
        "        tgen=poet.generate(1000,prompt)\n",
        "        # print(xso.replace(\"\\\\n\",\"\\n\"))\n",
        "        textlib.source_highlight(tgen, 10)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8mGGsuFRUlpi",
        "colab_type": "code",
        "outputId": "9b8cc8c6-b1de-4301-8d3a-677e26bebe06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter some dialog.\n",
            "The net will answer according to your input.\n",
            "'bye' for end,\n",
            "'reset' to reset the conversation context,\n",
            "    to change character of the dialog.\n",
            "\n",
            "> Good morning, my lady!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " spirit,<span style=\"background-color:#e2d7d5;\"> good to\r<br></span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">conceal.\"\r<br>\r<br></span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">\"And that's </span><sup>[3]</sup>o<span style=\"background-color:#eadbd8;\">nce in London</span><sup>[3]</sup>!\"<span style=\"background-color:#e2d7d5;\">\r<br>\r<br>Their con</span><sup>[4]</sup>dit<span style=\"background-color:#eadbd8;\">ery minute</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">s, looking </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">about to p</span><sup>[3]</sup>ass h<span style=\"background-color:#ebdef0;\">er degradation</span><sup>[2]</sup>;<span style=\"background-color:#e2d7d5;\"> and as happ</span><sup>[4]</sup>y the outcoven\r<br>on me! This<span style=\"background-color:#ebdef0;\"> with several </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">world, the</span><sup>[3]</sup>\r<br>yellow<span style=\"background-color:#ebdef0;\">ing among the </span><sup>[2]</sup>little<span style=\"background-color:#eadbd8;\"> book and </span><sup>[3]</sup>their\r<br>autominess steel<span style=\"background-color:#ebdef0;\">, but on the</span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">\r<br>great ha</span><sup>[4]</sup>lb, an ausl<span style=\"background-color:#ebdef0;\">on brought </span><sup>[2]</sup>back tea\r<br>giver o<span style=\"background-color:#e2d7d5;\">n, probably</span><sup>[4]</sup><span style=\"background-color:#d8daef;\"> a little change</span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">\r<br>to show </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">about Frank Churchill</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\"> in his voice</span><sup>[3]</sup>s<span style=\"background-color:#e2d7d5;\">\r<br>arranged.</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">\"\r<br>\r<br>\"Do you </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">suppose they </span><sup>[3]</sup>disting--or<span style=\"background-color:#d8daef;\">s to Mrs. </span><sup>[1]</sup>Church<span style=\"background-color:#eadbd8;\">. You're g</span><sup>[3]</sup>oing, (T<span style=\"background-color:#eadbd8;\">hat happened,</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">\r<br>and when </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">they are in</span><sup>[2]</sup> comfa<span style=\"background-color:#eadbd8;\">sting\r<br>to </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">sitting out.</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">\"\r<br>\r<br>\"High</span><sup>[4]</sup>eury--it takes?--<span style=\"background-color:#d8daef;\">What would you </span><sup>[1]</sup><span style=\"background-color:#d8daef;\">remember--</span><sup>[1]</sup>en<span style=\"background-color:#e2d7d5;\">. Knightley.\r<br></span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">I cannot imagine,</span><sup>[4]</sup> Joseph t<span style=\"background-color:#e2d7d5;\">ease to be</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">\r<br>comfortably</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">, except that </span><sup>[2]</sup>it did\r<br>do<span style=\"background-color:#eadbd8;\"> him. But it</span><sup>[3]</sup> so<span style=\"background-color:#ebdef0;\">led them i</span><sup>[2]</sup>t\r<br>ov<span style=\"background-color:#ebdef0;\">er immediate </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">them to te</span><sup>[3]</sup>eturity\r<br>as<span style=\"background-color:#ebdef0;\"> my letter </span><sup>[2]</sup><span style=\"background-color:#d8daef;\">is very ea</span><sup>[1]</sup><span style=\"background-color:#eadbd8;\">ting in his</span><sup>[3]</sup>\r<br>lieces, of song Hrom<span style=\"background-color:#e2d7d5;\"> spectacles.--</span><sup>[4]</sup>\r<br>Leok<span style=\"background-color:#e2d7d5;\"> a right to m</span><sup>[4]</sup>eet\r<br>put, for<span style=\"background-color:#eadbd8;\">, writing th</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">rough the r</span><sup>[3]</sup>i<span style=\"background-color:#eadbd8;\">ver\r<br>again</span><sup>[3]</sup>: Terening<span style=\"background-color:#ebdef0;\"> crossing </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">an end of </span><sup>[2]</sup>his cheek\r<br><span style=\"background-color:#e2d7d5;\">and I answered</span><sup>[4]</sup> so glass--making<span style=\"background-color:#e2d7d5;\">\r<br>Jane Fairfax's </span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">afternoon </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">Elizabeth to</span><sup>[4]</sup>\r<br>Sute (ca<span style=\"background-color:#d8daef;\">t one and </span><sup>[1]</sup>a "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "> That was the very sad story of her sudden death, after all that suffering.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:#ebdef0;\"> He glanced</span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">\r<br>at once t</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">ill after </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">her light--</span><sup>[2]</sup>but<span style=\"background-color:#e2d7d5;\"> Emma\r<br>reco</span><sup>[4]</sup>mmence<span style=\"background-color:#ebdef0;\">d resigned</span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">, she was g</span><sup>[4]</sup>o<span style=\"background-color:#eadbd8;\">ing to\r<br>dis</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">cover the s</span><sup>[3]</sup>mok<span style=\"background-color:#ebdef0;\">e stretched </span><sup>[2]</sup>from one\r<br>black<span style=\"background-color:#d8daef;\">. They took </span><sup>[1]</sup>al<span style=\"background-color:#e2d7d5;\">ters for her</span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">self\r<br>and </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">had indeed</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">. He was very </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">contentment.</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">\r<br>\r<br>\"Ah! I </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">mention you</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\"> have knocked </span><sup>[4]</sup><span style=\"background-color:#d8daef;\">it for Mr. </span><sup>[1]</sup><span style=\"background-color:#e2d7d5;\">Frank Churchill\r<br>a</span><sup>[4]</sup>lleath<span style=\"background-color:#eadbd8;\">les?\"\r<br>\r<br>\"</span><sup>[3]</sup>Who do<span style=\"background-color:#e2d7d5;\">es an excellent</span><sup>[4]</sup> point? I<span style=\"background-color:#ebdef0;\">t instead of </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">being\r<br>an a</span><sup>[3]</sup>rrogrite animal unlations\r<br>flou<span style=\"background-color:#ebdef0;\">ting; and </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">at least e</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">xercise it</span><sup>[4]</sup>\r<br><span style=\"background-color:#eadbd8;\">behind you. I</span><sup>[3]</sup>f _certainly_-I obeya<span style=\"background-color:#ebdef0;\">l\r<br>together</span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">. \"You don't </span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">read, and was</span><sup>[4]</sup>\r<br>dead<span style=\"background-color:#ebdef0;\">ed to see him </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">to seem to </span><sup>[2]</sup>her; the\r<br>embus<span style=\"background-color:#d8daef;\">tes of the </span><sup>[1]</sup>l<span style=\"background-color:#eadbd8;\">ank on the</span><sup>[3]</sup> plates of<span style=\"background-color:#d8daef;\">, before\r<br></span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">Heathcliff and H</span><sup>[2]</sup>irst. At Peoling,<span style=\"background-color:#e2d7d5;\">\r<br>practised </span><sup>[4]</sup>with<span style=\"background-color:#eadbd8;\"> green, and </span><sup>[3]</sup>his water<span style=\"background-color:#e2d7d5;\">-will\r<br>and </span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">village; but </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">as Miss Allan</span><sup>[3]</sup>\r<br>was sitisfi<span style=\"background-color:#e2d7d5;\">ed; and at </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">the idea of\r<br>Mr. </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">Woodhouse co</span><sup>[4]</sup>mels upon it!\r<br>\r<br> haming loud<span style=\"background-color:#e2d7d5;\"> than Emma a</span><sup>[4]</sup>bout.<span style=\"background-color:#e2d7d5;\"> Emma would</span><sup>[4]</sup>\r<br>di<span style=\"background-color:#ebdef0;\">d the sense of </span><sup>[2]</sup>unforment<span style=\"background-color:#ebdef0;\">ed insensible</span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">\r<br>before. </span><sup>[2]</sup>I'<span style=\"background-color:#ebdef0;\">d amusements</span><sup>[2]</sup><span style=\"background-color:#d8daef;\"> to tell you\r<br></span><sup>[1]</sup>_who,_<span style=\"background-color:#eadbd8;\"> and forget </span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">her believe</span><sup>[4]</sup><span style=\"background-color:#d8daef;\">d\r<br>sincere</span><sup>[1]</sup><span style=\"background-color:#eadbd8;\">ly. Her mind </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">had been a s</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">et out for\r<br></span><sup>[2]</sup>typ<span style=\"background-color:#e2d7d5;\">es; and so</span><sup>[4]</sup><span style=\"background-color:#d8daef;\"> that their </span><sup>[1]</sup>night<span style=\"background-color:#ebdef0;\">\r<br>moved by </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">Frank Churchill co</span><sup>[4]</sup>nfu"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "> bye\n",
            "Good bye!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKEqzxcvUlpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYEcftpVUlpp",
        "colab_type": "code",
        "outputId": "28be3dac-c4c2-49b3-d0cf-feeb43a203a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq9y6Wsg8RDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "07832f71-0441-4877-87e0-0f7f1ecf000b"
      },
      "source": [
        "!ls /content/drive/'My Drive'\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Archive\n",
            " Books\n",
            "'Colab Notebooks'\n",
            "'Copy of W&R - Adekyn.gsheet'\n",
            "'Google Photos'\n",
            " longchen1.gdoc\n",
            " longchen1.pdf\n",
            " Notes\n",
            "'ocr lens.png'\n",
            "'Play Books Notes'\n",
            "'Quantum Supremacy Using a Programmable Superconducting Processor.pdf'\n",
            "\"Recognizing Reality_ Dharmakirti's Philosophy and Its Tibetan Interpretations - Georges B. J. Dreyfus.pdf\"\n",
            "'RESTRICTION README.rtf.gdoc'\n",
            "'Screenshot (21.11.2019 12:15:02)'\n",
            " Λ-Engines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe-5qaobLwHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "694b1f2c-416e-422f-8ae8-9358b116abff"
      },
      "source": [
        "!cat sample_data/README.md"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This directory includes a few sample datasets to get you started.\n",
            "\n",
            "*   `california_housing_data*.csv` is California housing data from the 1990 US\n",
            "    Census; more information is available at:\n",
            "    https://developers.google.com/machine-learning/crash-course/california-housing-data-description\n",
            "\n",
            "*   `mnist_*.csv` is a small sample of the\n",
            "    [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is\n",
            "    described at: http://yann.lecun.com/exdb/mnist/\n",
            "\n",
            "*   `anscombe.json` contains a copy of\n",
            "    [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet); it\n",
            "    was originally described in\n",
            "\n",
            "    Anscombe, F. J. (1973). 'Graphs in Statistical Analysis'. American\n",
            "    Statistician. 27 (1): 17-21. JSTOR 2682899.\n",
            "\n",
            "    and our copy was prepared by the\n",
            "    [vega_datasets library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAgOsHF4LztF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}