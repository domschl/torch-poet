{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-poet/blob/master/torch_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28i44jSzUlon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from enum import Enum\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen  # Py3\n",
    "except:\n",
    "    print(\"This notebook requires Python 3.\")\n",
    "try:\n",
    "    import pathlib\n",
    "except:\n",
    "    print(\"At least python 3.5 is needed.\")\n",
    "    \n",
    "try: # Colab instance?\n",
    "    from google.colab import drive\n",
    "except: # Not? ignore.\n",
    "    pass\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eE-jm072kOKv"
   },
   "source": [
    "# 0. System configuration\n",
    "\n",
    "This notebook can either run on a local jupyter server, or on google cloud.\n",
    "If a GPU is available, it will be used for training (if `force_cpu` is not set to `True`).\n",
    "\n",
    "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
    "\n",
    "Similarily, the text corpora that are used for training, can be cached on drive or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVyGr-BCiJlR"
   },
   "outputs": [],
   "source": [
    "# force_cpu=True: use CPU for training, even if a GPU is available.\n",
    "#    Note: inference uses CPU always, because that is faster.\n",
    "force_cpu=False\n",
    "\n",
    "# Define where snapshots of training data are stored:\n",
    "colab_google_drive_snapshots=True\n",
    "\n",
    "# Define if training data (the texts downloaded from internet) are cached:\n",
    "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
    "local_jupyter_data_cache=True       # In local jupyter mode cache to local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ply0tFmz4O1E"
   },
   "outputs": [],
   "source": [
    "is_colab_notebook = 'google.colab' in sys.modules\n",
    "torch_version = torch.__version__\n",
    "\n",
    "if torch.cuda.is_available() and force_cpu is not True:\n",
    "    device='cuda'\n",
    "    use_cuda = True\n",
    "    print(f\"PyTorch {torch_version}, running on GPU\")\n",
    "    if is_colab_notebook:\n",
    "        card = !nvidia-smi\n",
    "        if len(card)>=8:\n",
    "            try:\n",
    "                gpu_type=card[7][6:25]\n",
    "                gpu_memory=card[8][33:54]\n",
    "                print(f\"Colab GPU: {gpu_type}, GPU Memory: {gpu_memory}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "else:\n",
    "    device='cpu'\n",
    "    use_cuda = False\n",
    "    print(f\"{torch_version}, running on CPU\")\n",
    "    if colab_notebook:\n",
    "        print(\"Note: on Google Colab, make sure to select:\")\n",
    "        print(\"      Runtime / Change Runtime Type / Hardware accelerator: GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTRmAT1f4O1I"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots:\n",
    "        mountpoint='/content/drive'\n",
    "        root_path='/content/drive/My Drive'\n",
    "        if not os.path.exists(root_path):\n",
    "            drive.mount(mountpoint)\n",
    "        if not os.path.exists(root_path):\n",
    "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
    "            colab_google_drive_snapshots=False\n",
    "    else:\n",
    "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
    "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
    "else:\n",
    "    root_path='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlMMykJxsJcv"
   },
   "outputs": [],
   "source": [
    "def one_hot(p, dim):\n",
    "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
    "    for y in range(p.shape[0]):\n",
    "        for x in range(p.shape[1]):\n",
    "            o[y,x,p[y,x]]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoutSg5IUlot"
   },
   "source": [
    "# 1. Text data collection\n",
    "\n",
    "**Important note:** the following `project_name` determines the root directory for training data and model snapshots, so it should be changed whenever datasets of model configurations are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xuuvAPeIjtp4"
   },
   "outputs": [],
   "source": [
    "project_name = \"philosophers_lang_eng\"\n",
    "project_description = \"A model trained on several books of philosophers in English language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPD_j8qRjtp7"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{project_name}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "else:\n",
    "    if local_jupyter_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"{project_name}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "\n",
    "if data_cache_path is not None:\n",
    "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(data_cache_path):\n",
    "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
    "            \n",
    "def get_cache_name(cache_path, author, title):\n",
    "    if cache_path is None:\n",
    "        return None\n",
    "    cname=f\"{author} - {title}.txt\"\n",
    "    cname=cname.replace('?','_')  # Gutenberg index is pre-Unicode-mess and some titles contain '?' for bad conversions.\n",
    "    cache_filepath=os.path.join(cache_path, cname)\n",
    "    return cache_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPkfXQbGjtp_"
   },
   "source": [
    "## 1.1 Project Gutenberg data source\n",
    "\n",
    "Search, filter, clean and download books from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gz_LqFccjtqA"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HELQbmZwjtqF"
   },
   "outputs": [],
   "source": [
    "class GutenbergLib:\n",
    "    \"\"\" A fuzzy, lightweight library to access, search and filter Project Gutenberg resources \"\"\"\n",
    "    def __init__(self, root_url=\"http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg\", cache_dir=\"gutenberg\"):\n",
    "        \"\"\" GutenbergLib by default uses a mirror's root URL\n",
    "        \n",
    "        root_url -- url of Project Gutenberg or any mirror URL.\n",
    "        cache_dir -- path to a directory that will be used to cache the Gutenberg index and already downloaded texts\n",
    "        \"\"\"\n",
    "        self.log = logging.getLogger('GutenbergLib')\n",
    "        self.root_url = root_url\n",
    "        self.index=None\n",
    "        self.NEAR=2048\n",
    "        try:\n",
    "            if not os.path.exists(cache_dir):\n",
    "                os.makedirs(cache_dir)\n",
    "            self.cache_dir=cache_dir\n",
    "        except Exception as e:\n",
    "            self.cache_dir=None\n",
    "            self.log.error(f\"Failed to create cache directory {cache_dir}, {e}\")\n",
    "\n",
    "    def _parse_record(self,record,verbose=True):\n",
    "        \"\"\" internal function to recreate some consistent record information from near-freestyle text \"\"\"\n",
    "        rl=record.split('\\n')\n",
    "        white=str(chr(160))+str(chr(9))+\" \" # non-breaking space, TAB, and space\n",
    "        ebook_no=\"\"\n",
    "        while len(rl[0])>0 and rl[0][-1] in white:\n",
    "            rl[0]=rl[0][:-1]\n",
    "        while len(rl[0])>0 and not rl[0][-1] in white:\n",
    "            ebook_no=rl[0][-1]+ebook_no\n",
    "            rl[0]=rl[0][:-1]\n",
    "        while len(rl[0])>0 and rl[0][-1] in white:\n",
    "            rl[0]=rl[0][:-1]\n",
    "        \n",
    "        # Sanity check\n",
    "        try:\n",
    "            fa=re.findall(ebook_no,\"\\A[0-9]+[A-C]\\Z\")\n",
    "        except Exception as e:\n",
    "            fa=None\n",
    "            if verbose is True:\n",
    "                self.log.debug(f\"Failed to apply regex on >{ebook_no}<\")\n",
    "            \n",
    "        if len(rl[0])<5 or fa==None or len(ebook_no)>7:\n",
    "            if verbose is True:\n",
    "                print(\"-------------------------------------\")\n",
    "                print(record)\n",
    "                print(\"- - - - - - - - - - - - - - - - - - -\")\n",
    "                print(f\"Dodgy record: {rl[0]}\")\n",
    "                print(f\"    ebook-id:  >{ebook_no}<\")\n",
    "            return None\n",
    "        \n",
    "        for i in range(len(rl)):\n",
    "            rl[i]=rl[i].strip()\n",
    "            \n",
    "        p=0\n",
    "        while p<len(rl)-1:\n",
    "            if len(rl[p+1])==0:\n",
    "                print(f\"Invalid rec: {record}\")\n",
    "                p+=1\n",
    "            else:\n",
    "                if rl[p+1][0]!=\"[\":\n",
    "                    rl[p]+=\" \"+rl[p+1]\n",
    "                    del rl[p+1]\n",
    "                    if rl[p][-1]==']':\n",
    "                        p+=1\n",
    "                else:\n",
    "                    p+=1\n",
    "        \n",
    "        rec={}\n",
    "        l0=rl[0].split(\", by \")\n",
    "        rec['title']=l0[0]\n",
    "        rec['ebook_id']=ebook_no\n",
    "        # if len(l0)>2:\n",
    "        #    print(f\"Chaos title: {rl[0]}\")\n",
    "        if len(l0)>1:\n",
    "            rec['author']=l0[-1]\n",
    "        for r in rl[1:]:\n",
    "            if r[0]!='[' or r[-1]!=']':\n",
    "                if r[0]=='[':\n",
    "                    ind=r.rfind(']')\n",
    "                    if ind != -1:\n",
    "                        # print(f\"Garbage trail {r}\")\n",
    "                        r=r[:ind+1]\n",
    "                        # print(f\"Fixed: {r}\")\n",
    "                    else:\n",
    "                        # print(f\"Missing closing ] {r}\")\n",
    "                        r+=']'\n",
    "                        # print(f\"Fixed: {r}\")\n",
    "            if r[0]=='[' and r[-1]==']':\n",
    "                r=r[1:-1]\n",
    "                i1=r.find(':')\n",
    "                if i1==-1:\n",
    "                    r=r.replace(\"Author a.k.a.\",\"Author a.k.a.:\")\n",
    "                    i1=r.find(':')\n",
    "                if i1!=-1:\n",
    "                    i2=r[i1:].find(' ')+i1\n",
    "                else:\n",
    "                    i2=-1\n",
    "                if i1==-1 and i2==-1:\n",
    "                    pass\n",
    "                    # print(f\"Invalid attribut in {rl}::{r}\")\n",
    "                else:\n",
    "                    if i2-i1==1:\n",
    "                        key=r[:i1]\n",
    "                        val=r[i2+1:]\n",
    "                        if '[' in key or ']' in key or '[' in val or ']' in val or len(key)>15:\n",
    "                            pass\n",
    "                            # print(\"messy key/val\")\n",
    "                        else:\n",
    "                            rec[key.strip().lower()]=val.strip()\n",
    "                    else:\n",
    "                        pass\n",
    "                        # print(f\"Bad attribute name terminator, missing ': ' {r}\")\n",
    "            else:\n",
    "                pass\n",
    "                # print(f\"Invalid attribut in {rl}::{r}\")\n",
    "        if len(rec)>1:\n",
    "            if \"language\" not in rec.keys():\n",
    "                rec[\"language\"]=\"English\"\n",
    "        return rec\n",
    "        \n",
    "    def _parse_index(self, lines):\n",
    "        \"\"\" internal function to parse the fuzzy text-based Gutenberg table of content \"\"\"\n",
    "        class State(Enum):\n",
    "            NONE=1,\n",
    "            SYNC_START=2,\n",
    "            SYNC_REC=3,\n",
    "            END=5\n",
    "    \n",
    "        white=str(chr(160))+str(chr(9))+\" \" # non-breaking space, TAB, and space\n",
    "        state=State.NONE\n",
    "        start_token=\"~ ~ ~ ~\"\n",
    "        stop_token=[\"=====\"]\n",
    "        end_token=\"<==End\"\n",
    "        ignore_headers=[\"TITLE and AUTHOR\"]\n",
    "        ignore_content=[\"Not in the Posted Archives\",\"human-read audio ebooks\", \"Audio:\"]\n",
    "        empty_lines=0\n",
    "        records=[]\n",
    "        for line in lines:\n",
    "            if line[:len(end_token)]==end_token:\n",
    "                state=State.END\n",
    "                break\n",
    "\n",
    "            if state==State.NONE:\n",
    "                if line[:len(start_token)]==start_token:\n",
    "                    state=State.SYNC_START\n",
    "                    empty_lines=0\n",
    "                    continue\n",
    "            if state==State.SYNC_START:\n",
    "                if len(line.strip())==0:\n",
    "                    empty_lines+=1\n",
    "                    if empty_lines>1:\n",
    "                        state=State.NONE\n",
    "                        continue\n",
    "                else:\n",
    "                    stopped=False\n",
    "                    for stop in stop_token:\n",
    "                        if line[:len(stop)]==stop:\n",
    "                            stopped=True\n",
    "                            break\n",
    "                    if stopped is True:\n",
    "                        state=State.NONE\n",
    "                        empty_lines=0\n",
    "                        continue\n",
    "                    ignore=False\n",
    "                    for header in ignore_headers:\n",
    "                        if line[:len(header)]==header:\n",
    "                            empty_lines=0\n",
    "                            ignore=True\n",
    "                    for token in ignore_content:\n",
    "                        if token in line:\n",
    "                            empty_lines=0\n",
    "                            ignore=True\n",
    "                    if ignore is True:\n",
    "                        continue\n",
    "                    rec=line\n",
    "                    state=State.SYNC_REC\n",
    "                    continue\n",
    "            if state==State.SYNC_REC:\n",
    "                if len(line.strip())==0 or line[0] not in white:\n",
    "                    if len(records)<10:\n",
    "                        parsed_rec=self._parse_record(rec, verbose=True)\n",
    "                    else:\n",
    "                        parsed_rec=self._parse_record(rec, verbose=False)\n",
    "                        \n",
    "                    if parsed_rec is not None:\n",
    "                        records.append(parsed_rec)\n",
    "                    empty_lines=1\n",
    "                    if len(line.strip())==0:\n",
    "                        state=State.SYNC_START\n",
    "                        continue\n",
    "                    else:\n",
    "                        rec=line\n",
    "                        continue\n",
    "                rec=rec+\"\\n\"+line\n",
    "        return records\n",
    "                    \n",
    "    def load_index(self, cache=True, cache_expire_days=30):\n",
    "        \"\"\" This function loads the Gutenberg record index, either from cache, or from a website\n",
    "        \n",
    "        cache -- default True, use the cache directory to cache both index and text files. Index\n",
    "        expires after cache_expire_days, text files never expire. Should *NOT* be set to False\n",
    "        in order to prevent unnecessary re-downloading.\n",
    "        cache_expire_days -- Number of days after which the index is re-downloaded.\"\"\"\n",
    "        raw_index=None\n",
    "        if self.cache_dir is None:\n",
    "            self.log.error(\"Cannot cache library index, no valid cache directory.\")\n",
    "            return False\n",
    "        ts_file=os.path.join(self.cache_dir,\"timestamp\")\n",
    "        cache_file=os.path.join(self.cache_dir,\"gutenberg_index\")\n",
    "        expired=True\n",
    "        read_from_cache=False\n",
    "        if os.path.isfile(ts_file) and os.path.isfile(cache_file):\n",
    "            try:\n",
    "                with open(ts_file,'r') as f:\n",
    "                    ts=float(f.read())\n",
    "                if time.time()-ts<cache_expire_days*24*3600:\n",
    "                    expired=False\n",
    "                    read_from_cache = True\n",
    "                    self.log.debug(\"Cache timestamp read.\")\n",
    "                else:\n",
    "                    self.log.debug(\"Cache for index is expired, reloading from web.\")\n",
    "            except:\n",
    "                self.log.debug(\"Failed to read cache timestamp, reloading from web.\")\n",
    "        if expired is False and os.path.isfile(cache_file):\n",
    "            try:\n",
    "                with open(cache_file,'r') as f:\n",
    "                    raw_index=f.read()\n",
    "                    self.log.info(f\"Gutenberg index read from {cache_file}\")\n",
    "            except:\n",
    "                expired=True\n",
    "                self.log.debug(\"Failed to read cached index, reloading from web.\")\n",
    "        if expired is True:\n",
    "            index_url=self.root_url+\"/GUTINDEX.ALL\"\n",
    "            try:\n",
    "                raw_index = urlopen(index_url).read().decode('utf-8')\n",
    "                if raw_index[0]=='\\ufeff':  # Ignore BOM\n",
    "                    raw_index=raw_index[1:]\n",
    "                raw_index=raw_index.replace('\\r','')\n",
    "                self.log.info(f\"Gutenberg index read from {index_url}\")\n",
    "            except Exception as e:\n",
    "                self.log.error(f\"Failed to download Gutenberg index from {index_rul}, {e}\")\n",
    "                return False\n",
    "        if cache is True and read_from_cache is False:\n",
    "            try:\n",
    "                with open(ts_file,'w') as f:\n",
    "                    f.write(str(time.time()))\n",
    "                    self.log.debug(\"Wrote read cache timestamp.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write cache timestamp to {ts_file}, {e}\")\n",
    "            try:\n",
    "                with open(cache_file,'w') as f:\n",
    "                    f.write(raw_index)\n",
    "                    self.log.debug(\"Wrote read cached index.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to write cached index to {cache_file}, {e}\")\n",
    "        lines=raw_index.split('\\n')\n",
    "        self.records=self._parse_index(lines)\n",
    "    \n",
    "    def load_book(self, ebook_id):\n",
    "        \"\"\" get text of an ebook from Gutenberg by ebook_id \n",
    "        \n",
    "        ebook_id -- Gutenberg id\n",
    "        \"\"\"\n",
    "        if ebook_id is None or len(ebook_id)==0:\n",
    "            return None\n",
    "        if ebook_id[-1]=='C':\n",
    "            ebook_id=ebook_id[:-1]\n",
    "        path_stub=\"\"\n",
    "        \n",
    "        for i in range(len(ebook_id)-1):\n",
    "            path_stub+=\"/\"+ebook_id[i]\n",
    "        path_stub+=\"/\"+ebook_id+\"/\"\n",
    "        filenames=[(ebook_id+\"-0.txt\",'utf-8'), (ebook_id+\".txt\",'utf-8'), (ebook_id+\"-8.txt\",\"latin1\")]\n",
    "        cache_name=ebook_id+\".txt\"\n",
    "        if self.cache_dir is not None:\n",
    "            cache_file=os.path.join(self.cache_dir,cache_name)\n",
    "            if os.path.isfile(cache_file):\n",
    "                try:\n",
    "                    with open(cache_file,'r') as f:\n",
    "                        data=f.read()\n",
    "                        self.log.info(f\"Book read from cache at {cache_file}\")\n",
    "                        return data\n",
    "                except Exception as e:\n",
    "                    self.log.error(f\"Failed to read cached file {cache_file}\")\n",
    "        data=None\n",
    "        for filename, encoding in filenames:\n",
    "            file_url=self.root_url+path_stub+filename\n",
    "            try:\n",
    "                data = urlopen(file_url).read().decode(encoding)\n",
    "                self.log.info(f\"Book downloaded from {file_url}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                self.log.debug(f\"URL-Download failed: {file_url}, {e}\")\n",
    "                pass\n",
    "        if data is None:\n",
    "            self.log.error(f\"Failed to download {filenames}\")\n",
    "            return None\n",
    "        if self.cache_dir is not None:\n",
    "            try:\n",
    "                with open(cache_file,'w') as f:\n",
    "                    f.write(data)\n",
    "            except:\n",
    "                self.log.error(f\"Failed to cache file {cache_file}\")\n",
    "        return data\n",
    "    \n",
    "    def filter_text(self, book_text):\n",
    "        \"\"\" Heuristically remove header and trailer texts not part of the actual book \n",
    "        \"\"\"\n",
    "        start_tokens=[\"*** START OF THIS PROJECT\", \"E-text prepared by\", \"This book was generously provided by the \"]\n",
    "        near_start_tokens=[\"produced by \", \"Produced by \", \"Transcriber's Note\", \"Transcriber's note:\", \"Anmerkungen zur Tanskription\"]\n",
    "        end_tokens=[\"End of the Project Gutenberg\", \"*** END OF THIS PROJECT\", \"***END OF THE PROJECT GUTENBER\",\n",
    "                   \"Ende dieses Projekt Gutenberg\", \"End of Project Gutenberg\", \"Transcriber's Note\"]\n",
    "        blen=len(book_text)\n",
    "        \n",
    "        pstart=0\n",
    "        for token in start_tokens:\n",
    "            pos=book_text.find(token)\n",
    "            if pos > pstart:\n",
    "                pstart = pos\n",
    "                self.log.debug(f\"Start-token [{token}] found at position {pos}\")\n",
    "        if pstart>0:\n",
    "            pos=book_text[pstart:].find(\"\\n\\n\")\n",
    "            if pos>=0 and pos <= self.NEAR:\n",
    "                pos += pstart\n",
    "                while book_text[pos]=='\\n':\n",
    "                    pos += 1  # eof?!\n",
    "                pstart=pos\n",
    "        if pstart>blen/2:\n",
    "            self.log.warning(\"Preamble is taking more than half of the book!\")\n",
    "        new_book=book_text[pstart:]\n",
    "        \n",
    "        xpos=-1\n",
    "        for token in near_start_tokens:\n",
    "            pos=new_book.find(token)\n",
    "            if pos>=0 and pos<=self.NEAR:\n",
    "                self.log.debug(f\"Near-Start-token [{token}] found at position {pos}\")\n",
    "                if pos>xpos:\n",
    "                    xpos=pos\n",
    "        if xpos > -1:\n",
    "            pos2=new_book[xpos:].find(\"\\n\\n\")\n",
    "            self.log.debug(f\"Trying extra skipping for {pos2}...\")\n",
    "            if pos2<=self.NEAR and pos2>0:\n",
    "                self.log.debug(\"Trying extra skipping (2)...\")\n",
    "                while new_book[xpos+pos2]=='\\n':\n",
    "                    pos2 += 1\n",
    "                new_book=new_book[xpos+pos2:]\n",
    "                self.log.debug(f\"Additionally shortened start by {xpos+pos2} chars\")\n",
    "        \n",
    "        pend=len(new_book)\n",
    "        for token in end_tokens:\n",
    "            pos=new_book.find(token)\n",
    "            if pos!=-1 and pos < pend:\n",
    "                self.log.debug(f\"End-token [{token}] found at pos {pos}\")\n",
    "                pend = pos\n",
    "        if pend<len(new_book):\n",
    "            pos=new_book[:pend].rfind(\"\\n\\n\")\n",
    "            if pos>0:\n",
    "                while new_book[pos]=='\\n':\n",
    "                    pos -= 1  # eof?!\n",
    "                pend=pos+1\n",
    "        else:\n",
    "            self.log.debug(\"No end token found!\")\n",
    "        if pend<len(new_book)/2:\n",
    "            self.log.warning(\"End-text is taking more than half of the book!\")\n",
    "        new_book=new_book[:pend]\n",
    "        return new_book\n",
    "        \n",
    "    def find_keywords(self,*search_keys):\n",
    "        \"\"\" Search of an arbitrary number of keywords in a book record\n",
    "        \n",
    "        returns -- list of records that contain all keywords in any field. \"\"\"\n",
    "        frecs=[]\n",
    "        for rec in self.records:\n",
    "            found=True\n",
    "            for sk in search_keys:\n",
    "                subkey=False\n",
    "                for key in rec.keys():\n",
    "                    if sk.lower() in key.lower() or sk.lower() in rec[key].lower():\n",
    "                        subkey=True\n",
    "                        break\n",
    "                if subkey is False:\n",
    "                    found=False\n",
    "                    break\n",
    "            if found is True:\n",
    "                frecs += [rec]\n",
    "        return frecs\n",
    "    \n",
    "    def search(self, search_dict):\n",
    "        \"\"\" Search for book record with key specific key values\n",
    "        For a list of valid keys, use `get_record_keys()`\n",
    "        Standard keys are:\n",
    "        ebook_id, author, language, title\n",
    "        example: search({\"title\": [\"philosoph\",\"phenomen\",\"physic\",\"hermeneu\",\"logic\"], \"language\":\"english\"})\n",
    "        Find all books whose titles contain at least one the keywords, language english. Search keys can either be\n",
    "        search for a single keyword (e.g. english), or an array of keywords. \n",
    "        returns -- list of records \"\"\"\n",
    "        frecs=[]\n",
    "        for rec in self.records:\n",
    "            found=True\n",
    "            for sk in search_dict:\n",
    "                if sk not in rec:\n",
    "                    found=False\n",
    "                    break\n",
    "                else:\n",
    "                    skl=search_dict[sk]\n",
    "                    if not isinstance(skl,list):\n",
    "                        skl=[skl]\n",
    "                    nf=0\n",
    "                    for skli in skl:\n",
    "                        if skli.lower() in rec[sk].lower():\n",
    "                            nf=nf+1\n",
    "                    if nf==0:\n",
    "                        found=False\n",
    "                        break\n",
    "            if found is True:\n",
    "                frecs += [rec]\n",
    "        return frecs\n",
    "        \n",
    "    \n",
    "    def get_record_keys(self):\n",
    "        \"\"\" Get a list of all keys that are used within records. Standard keys are:\n",
    "        ebook_id, author, language, title\n",
    "        \n",
    "        returns -- list of all different keys that are somehow used.\"\"\"\n",
    "        rks=[]\n",
    "        for r in self.records:\n",
    "            rks=set(list(rks) + list(r.keys()))\n",
    "        return rks\n",
    "\n",
    "    def get_unique_record_values(self, key):\n",
    "        \"\"\" Get a list of all unique values a given keys has for all records.\n",
    "        get_unique_records_values('language') returns all languages in Gutenberg.\"\"\"\n",
    "        uv=[]\n",
    "        if key not in self.get_record_keys():\n",
    "            print(f\"{key} is not a key used in any record!\")\n",
    "            return None\n",
    "        for r in self.records:\n",
    "            if key in r:\n",
    "                uv=set(list(uv)+[r[key]])\n",
    "        uv=sorted(uv)\n",
    "        return uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3C1yExTjtqI"
   },
   "outputs": [],
   "source": [
    "# Get the list of available books on Gutenberg.\n",
    "gbl=GutenbergLib(cache_dir=os.path.join(root_path, 'gutenberg_cache'))\n",
    "gbl.load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hrI3xSK7jtqL"
   },
   "outputs": [],
   "source": [
    "# sample searches\n",
    "search_specs=[\n",
    "    {\"title\": [\"love\", \"hate\", \"emotion\", \"drama\"], \"language\": [\"english\"]},\n",
    "    {\"author\": [\"brontë\",\"Jane Austen\", \"Woolf\", \"goethe\", \"kant\"], \"language\": [\"english\", \"german\"]},\n",
    "    {\"title\": [\"philosoph\", \"physic\", \"phenomen\", \"logic\"], \"language\": [\"english\"]},\n",
    "]\n",
    "for search_spec in search_specs:\n",
    "    book_list=gbl.search(search_spec)\n",
    "    print(f\"{len(book_list)} matching books found with search {search_spec}.\")\n",
    "# a search spec can be used by the following text library as datasource, it will automatically download, filter and prepare the content of the books requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqKMN8nLjtqO"
   },
   "outputs": [],
   "source": [
    "def create_libdesc(project_name, description, cache_path, book_list):\n",
    "    libdesc={\"name\": project_name, \"description\": description, \"lib\": []}\n",
    "    if cache_path is None or not os.path.exists(cache_path):\n",
    "        print(f\"A valid cache {cache_path} is needed!\")\n",
    "        return None\n",
    "    for book_entry in book_list:\n",
    "        try:\n",
    "            book_raw_content=gbl.load_book(book_entry['ebook_id'])\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download ebook_id {book_entry}, {e}\")\n",
    "            continue\n",
    "        if book_raw_content is not None:\n",
    "            try:\n",
    "                book_text=gbl.filter_text(book_raw_content)\n",
    "            except Exception as e:\n",
    "                print(f\"Internal error when filtering {book_entry}, {e}\")\n",
    "                continue\n",
    "            filename=get_cache_name(cache_path, book_entry['author'], book_entry['title'])\n",
    "            try:\n",
    "                with open(filename,'w') as f:\n",
    "                    f.write(book_text)\n",
    "                    print(f\"Cached {filename}\")\n",
    "                    libdesc[\"lib\"].append((filename, book_entry['author'], book_entry['title']))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to cache {filename}\", {e})\n",
    "    return libdesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQo6nTsPjtqR"
   },
   "outputs": [],
   "source": [
    "book_list=gbl.search({\"author\": [\"platon\", \"descartes\", \"john locke\", \"david hume\", \"kant\", \"schopenhauer\", \"leibniz\", \"kierkegaard\", \"hegel\", \"nietzsche\", \"heidegger\", \"fichte\"], \"language\": [\"english\"]})\n",
    "print(f\"{len(book_list)} books found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnYGn0LylSyo"
   },
   "outputs": [],
   "source": [
    "book_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tm97yp2ijtqV"
   },
   "outputs": [],
   "source": [
    "# this will download the books! make sure it's a reasonable number of books\n",
    "libdesc=create_libdesc(project_name, project_description, data_cache_path, book_list)\n",
    "\n",
    "with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
    "    json.dump(libdesc,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LF4CGmsbjtqY"
   },
   "outputs": [],
   "source": [
    "libdesc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXqlcaNajtqb"
   },
   "source": [
    "## 1.2 Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts of a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzXrUVe-4O1N"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pMoAJ-SVDm2"
   },
   "outputs": [],
   "source": [
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.cache_dir=text_data_cache_directory\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        index = 1\n",
    "        for descriptor, author, title in descriptors:\n",
    "            fd = {}\n",
    "            cache_name=get_cache_name(self.cache_dir, author, title)\n",
    "            if os.path.exists(cache_name):\n",
    "                is_cached=True\n",
    "            else:\n",
    "                is_cached=False\n",
    "            valid=False\n",
    "            if descriptor[:4] == 'http' and is_cached is False:\n",
    "                try:\n",
    "                    print(f\"Downloading {cache_name}\")\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    dat=dat.replace('\\r', '')  # get rid of pesky LFs \n",
    "                    self.data += dat\n",
    "                    fd[\"title\"] = title\n",
    "                    fd[\"author\"] = author\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    valid=True\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"title\"] = title\n",
    "                fd[\"author\"] = author\n",
    "                try:\n",
    "                    if is_cached is True:\n",
    "                        print(f\"Reading {cache_name} from cache\")\n",
    "                        f = open(cache_name)\n",
    "                    else:    \n",
    "                        f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                    valid=True\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
    "                try:\n",
    "                    print(f\"Caching {cache_name}\")\n",
    "                    f = open(cache_name, 'w')\n",
    "                    f.write(dat)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
    "                \n",
    "                \n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def display_colored_html(self, textlist, dark_mode=False, display_ref_anchor=True, pre='', post=''):\n",
    "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
    "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
    "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
    "        if dark_mode is False:\n",
    "            bgcolors=bgcolorsWht\n",
    "        else:\n",
    "            bgcolors=bgcolorsDrk\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                if display_ref_anchor is True:\n",
    "                    anchor=\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "                else:\n",
    "                    anchor=\"\"\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+ anchor\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False, display_ref_anchor=True):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f\"{f['author']}: {f['title']}\"\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out, dark_mode=dark_mode, display_ref_anchor=display_ref_anchor)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, dark_mode=dark_mode, display_ref_anchor=display_ref_anchor, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.c2i[c] for c in s]\n",
    "        \n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return np.array(smpX), np.array(smpy)\n",
    "    \n",
    "    def get_random_onehot_sample_batch(self, batch_size, length):\n",
    "        X, y = self.get_random_sample_batch(batch_size, length)\n",
    "        return one_hot(X,len(self.i2c)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcMGC5KDUloz"
   },
   "source": [
    "## 1.3 Data sources\n",
    "\n",
    "Data sources can either be:\n",
    "\n",
    "1. files from local filesystem, or for colab notebooks from google drive, \n",
    "2. http(s) links\n",
    "\n",
    "The `name` given will be use as directory name for both snapshots and model data caches.\n",
    "\n",
    "Each entry in the `lib` array contains of:\n",
    "\n",
    "1. (1) a local filename or (2) https(s) link\n",
    "2. an Author's name\n",
    "3. a title\n",
    "\n",
    "Samples: (we are using the `libdesc` created above from `GutenbergLib`\n",
    "```\n",
    "libdesc = {\n",
    "    \"name\": \"Women-Writers\",\n",
    "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
    "    \"lib\": [\n",
    "        # local file:\n",
    "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),\n",
    "\n",
    "        # http URLs:\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', 'Virginia Woolf', 'Voyage out'),\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', 'Jane Austen', 'Emma'),\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTg4QFewsJdE"
   },
   "outputs": [],
   "source": [
    "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLibraryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, textlib, sample_length, text_quanta=10, encode=True):\n",
    "        self.textlib=textlib\n",
    "        self.vocab_size=len(textlib.i2c)\n",
    "        self.encode=encode\n",
    "        self.text_quanta=text_quanta\n",
    "        self.sample_length=sample_length\n",
    "        self.length=int((len(self.textlib.data)-sample_length-1)/text_quanta)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "#        if type(idx)==list:\n",
    "#            ans=[]\n",
    "#            for id in idx:\n",
    "#                if id>=0 and id<self.length:\n",
    "#                    if self.encode is True:\n",
    "#                        ansi=textlib.encode(self.textlib.data[id*self.text_quanta:id*self.text_quanta+self.sample_length])\n",
    "#                    else:\n",
    "#                        ansi=self.textlib.data[id*self.text_quanta:id*self.text_quanta+self.sample_length]                        \n",
    "#                    ans.append(ansi)\n",
    "#            return ans\n",
    "#        else:\n",
    "        if idx>=self.length:\n",
    "            return None\n",
    "        if self.encode is True:\n",
    "            ansi=textlib.encode(self.textlib.data[idx*self.text_quanta:idx*self.text_quanta+self.sample_length+1])\n",
    "            # X = Tensor(torch.from_numpy(one_hot(ansi[:-1], self.vocab_size)))\n",
    "            aX=ansi[:-1].copy()\n",
    "            ay=ansi[1:].copy()\n",
    "            X = Tensor(torch.from_numpy(np.array(one_hot(np.array([aX], dtype=np.int32), self.vocab_size)[0],dtype=np.float32)))\n",
    "            y = torch.LongTensor(torch.from_numpy(np.array(ay,dtype=np.int64)))\n",
    "        else:\n",
    "            ansi=self.textlib.data[idx*self.text_quanta:idx*self.text_quanta+self.sample_length+1]                        \n",
    "            X, y=(ansi[:-1], ansi[1:])\n",
    "        return X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zP0Hcs82lWYI"
   },
   "source": [
    "# 2. The deep LSTM model\n",
    "\n",
    "# 2.1 Model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEM7Y3GxUlo0"
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"model_name\": libdesc['name'],\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 256,\n",
    "    \"layers\": 2,\n",
    "    \"learning_rate\": 1.e-3,\n",
    "    \"steps\": 60,\n",
    "    \"batch_size\": 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_JWmbgPUlpG"
   },
   "source": [
    "## 2.2 The char-rnn model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcsP8OYlUlpH"
   },
   "outputs": [],
   "source": [
    "class Poet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
    "        super(Poet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.device=device\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
    "        \n",
    "        self.demb = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputx, steps):\n",
    "        self.lstm.flatten_parameters()\n",
    "        hn, (self.h0, self.c0) = self.lstm(inputx.to(self.device), (self.h0, self.c0))\n",
    "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
    "        op = self.demb(hnr)\n",
    "        opr = op.view(-1, steps ,self.output_size)\n",
    "        return opr\n",
    "\n",
    "    def generate(self, n, start=None, temperature=1.0):\n",
    "        s=''\n",
    "        torch.set_grad_enabled(False)\n",
    "        if start==None or len(start)==0:\n",
    "            start=' '\n",
    "        self.init_hidden(1)\n",
    "        for c in start:\n",
    "            X=np.array([[textlib.c2i[c]]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            if temperature>0.0:\n",
    "                ypl2 = ypl2 / temperature\n",
    "            yp = self.softmax(ypl2)\n",
    "        for i in range(n):\n",
    "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
    "            y_pred=ypc.numpy()\n",
    "            inds=list(range(self.output_size))\n",
    "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
    "            s=s+textlib.i2c[ind]\n",
    "            X=np.array([[ind]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            if temperature>0.0:\n",
    "                ypl2 = ypl2 / temperature\n",
    "            yp = self.softmax(ypl2)\n",
    "        torch.set_grad_enabled(True)\n",
    "        return s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymdU_wIWUlpK"
   },
   "source": [
    "## 2.3 Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7WuQ142UlpL"
   },
   "outputs": [],
   "source": [
    "poet = Poet(model_params['vocab_size'], model_params['neurons'], model_params['layers'], model_params['vocab_size'], device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJp6Sjw6UlpP"
   },
   "source": [
    "## 2.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeFxMxyuUlpQ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = model_params['learning_rate']\n",
    "\n",
    "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtKdvxNymXpM"
   },
   "source": [
    "## 2.5 Helper Functions\n",
    "\n",
    "These allow to save or restore the training data. Saving and restoring can either be performed:\n",
    "\n",
    "* Jupyter: store/restore in a local directory,\n",
    "* Colab: store/restore on google drive. The training-code (using load_checkpoint()) will display an authentication url and code input-box in order to be able to access your google drive from this notebook. This allows to continue training sessions (or inference) after the Colab session was terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots is True:\n",
    "        snapshot_path=os.path.join(root_path,f\"Colab Notebooks/{model_params['model_name']}/Snapshots\")\n",
    "    else:\n",
    "        snapshot_path=None\n",
    "else:\n",
    "    snapshot_path=os.path.join(root_path,f\"{model_params['model_name']}/Snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuVlJksgsJde"
   },
   "outputs": [],
   "source": [
    "def get_project_path():\n",
    "    if snapshot_path is None:\n",
    "        return None\n",
    "    project_path_ext=f\"model-{model_params['vocab_size']}x{model_params['steps']}x{model_params['layers']}x{model_params['neurons']}\"\n",
    "    return os.path.join(snapshot_path, project_path_ext)\n",
    "\n",
    "def create_project_path():\n",
    "    if snapshot_path is None:\n",
    "        return None\n",
    "    ppath=get_project_path()\n",
    "    pathlib.Path(ppath).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QebezmDgsJdh"
   },
   "outputs": [],
   "source": [
    "if snapshot_path is not None:\n",
    "    pathlib.Path(snapshot_path).mkdir(parents=True, exist_ok=True)\n",
    "    create_project_path()\n",
    "    with open(os.path.join(get_project_path(),'model_params.json'),'w') as f:\n",
    "        json.dump(model_params,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3v82UHQsJdj"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, loss, pr, best_pr, filename='checkpoint.pth.tar'):\n",
    "    if snapshot_path is None:\n",
    "        return\n",
    "    state={\n",
    "            'epoch': epoch,\n",
    "            'model_config': model_params,\n",
    "            'state_dict': poet.state_dict(),\n",
    "            'optimizer' : opti.state_dict(),\n",
    "            'precision': pr,\n",
    "            'loss': loss,\n",
    "        }\n",
    "    project_path=get_project_path()\n",
    "    save_file=os.path.join(project_path,filename)\n",
    "    best_file=os.path.join(project_path,'model_best.pth.tar')\n",
    "    torch.save(state, save_file)\n",
    "    if pr>best_pr:\n",
    "        best_pr=pr\n",
    "        shutil.copyfile(save_file, best_file )\n",
    "        print(f\"Saved best precision model, prec={pr}\")\n",
    "    else:\n",
    "        print(f\"saved last model data, prec={pr}\")\n",
    "\n",
    "def save_history(history, filename=\"history.json\"):\n",
    "    if snapshot_path is None:\n",
    "        return\n",
    "    project_path=get_project_path()\n",
    "    save_file=os.path.join(project_path,filename)\n",
    "    try:\n",
    "        with open(save_file, 'w') as f:\n",
    "            json.dump(history, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write training history file {save_file}, {e}\")\n",
    "\n",
    "def load_history(filename=\"history.json\"):\n",
    "    if snapshot_path is None:\n",
    "        return [], time.time()\n",
    "    project_path=get_project_path()\n",
    "    load_file=os.path.join(project_path,filename)\n",
    "    try:\n",
    "        with open(load_file, 'r') as f:\n",
    "            history=json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Starting new history file {load_file}\")\n",
    "        return [], time.time()\n",
    "    if len(history)>0:\n",
    "        start=history[-1][\"timestamp\"]\n",
    "    return history, start\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    if snapshot_path is None:\n",
    "        return 0,0\n",
    "    project_path=get_project_path()\n",
    "    load_file=os.path.join(project_path,filename)\n",
    "    if not os.path.exists(load_file):\n",
    "        print(load_file)\n",
    "        print(\"No saved state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    state=torch.load(load_file)\n",
    "    mod_conf = state['model_config']\n",
    "    if (mod_conf['model_name']!=model_params['model_name']):\n",
    "        print(f\"Warning: project has been renamed from {mod_conf['model_name']} to {model_param['model_name']}\")\n",
    "        mod_conf['model_name']=model_params['model_name']\n",
    "    if model_params!=mod_conf:\n",
    "        print(f\"The saved model has a different configuration than the current model: {mod_conf} vs. {model_params}\")\n",
    "        print(\"Cannot restore state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    poet.load_state_dict(state['state_dict'])\n",
    "    opti.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    loss = state['loss']\n",
    "    best_pr = state['precision']\n",
    "    print(f\"Continuing from saved state epoch={epoch}, loss={loss}\")  # Save is not necessarily on epoch boundary, so that's approx.\n",
    "    return epoch,loss\n",
    "\n",
    "# def one_hot(p, dim):\n",
    "#     o=np.zeros(p.shape+(dim,), dtype=int32)\n",
    "#     for y in range(p.shape[0]):\n",
    "#         for x in range(p.shape[1]):\n",
    "#             o[y,x,p[y,x]]=1\n",
    "#     return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_-ISha4nqhy"
   },
   "source": [
    "# 3. Training\n",
    "\n",
    "If there is already saved training data, this step is optional, and alternatively, ch. 4 can be continued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KDRpEm0n7B-"
   },
   "source": [
    "## 3.1 Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pZlsZ1Pnm5Z"
   },
   "outputs": [],
   "source": [
    "def torch_data_loader(batch_size, sample_length):\n",
    "    textlib_dataset=TextLibraryDataset(textlib,sample_length,True)\n",
    "    data_loader=torch.utils.data.DataLoader(textlib_dataset,batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    return data_loader\n",
    "\n",
    "# def get_data(use_torch_dataloader=True):\n",
    "#     return X, y\n",
    "\n",
    "def train(Xt, yt, bPr=False):\n",
    "    poet.zero_grad()\n",
    "\n",
    "    poet.init_hidden(Xt.size(0))\n",
    "    output = poet(Xt, model_params['steps'])\n",
    "    \n",
    "    olin=output.view(-1,model_params['vocab_size'])\n",
    "    _, ytp=torch.max(olin,1)\n",
    "    ytlin=yt.view(-1)\n",
    "\n",
    "    pr=0.0\n",
    "    if bPr: # Calculate precision\n",
    "        ok=0\n",
    "        nok=0\n",
    "        for i in range(ytlin.size()[0]):\n",
    "            i1=ytlin[i].item()\n",
    "            i2=ytp[i].item()\n",
    "            if i1==i2:\n",
    "                ok = ok + 1\n",
    "            else:\n",
    "                nok = nok+1\n",
    "            pr=ok/(ok+nok)\n",
    "            \n",
    "    loss = criterion(olin, ytlin)\n",
    "    ls = loss.item()\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "\n",
    "    return ls, pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC36hNRKUlpU"
   },
   "source": [
    "## 3.2 The actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q9-3GUQ4UlpV",
    "outputId": "6c121dbf-2ac0-4a0a-bdb3-8ce646b95f30",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls=0\n",
    "nrls=0\n",
    "\n",
    "create_project_path()\n",
    "epoch_start, _ = load_checkpoint()\n",
    "history, start_time = load_history()\n",
    "pr=0.0\n",
    "best_pr=0.0\n",
    "\n",
    "use_data_loader=True\n",
    "if use_data_loader is True:\n",
    "    data_loader=torch_data_loader(model_params['batch_size'], model_params['steps'])\n",
    "    \n",
    "# Make a snapshot of the trained parameters every snapshot_interval_sec\n",
    "snapshot_interval_sec=300\n",
    "# Generate text samples every sample_intervall_sec\n",
    "sample_interval_sec=600\n",
    "\n",
    "last_snapshot=time.time()\n",
    "last_sample=time.time()\n",
    "\n",
    "bench_all=0\n",
    "bench_data=0\n",
    "bench_train=0\n",
    "bench_train_withprec=0\n",
    "bench_sample=0\n",
    "bench_snapshot=0\n",
    "\n",
    "for e in range(epoch_start,2500000):\n",
    "    t0=time.time()\n",
    "    \n",
    "    t1=time.time()\n",
    "    for Xi,yi in data_loader:\n",
    "        #     Xo, yo=textlib.get_random_onehot_sample_batch(model_params['batch_size'], model_params['steps'])\n",
    "        #     X = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32)))\n",
    "        #     y = torch.LongTensor(torch.from_numpy(np.array(yo,dtype=np.int64)))\n",
    "        # X=X.to(device)\n",
    "        # y=y.to(device)\n",
    "        # X.requires_grad_(False)\n",
    "        # y.requires_grad_(False)\n",
    "        # Xt, yt = get_data(use_data_loader)\n",
    "        Xt=Xi.to(device)\n",
    "        yt=yi.to(device)\n",
    "        Xt.requires_grad_(False)\n",
    "        yt.requires_grad_(False)\n",
    "        bench_data += time.time()-t1\n",
    "        if time.time()-last_snapshot > snapshot_interval_sec:\n",
    "            t1=time.time()\n",
    "            l, pr = train(Xt,yt,True)\n",
    "            if pr>best_pr:\n",
    "                best_pr=pr\n",
    "            bench_train_withprec+=time.time()-t1\n",
    "        else:\n",
    "            t1=time.time()\n",
    "            l, _ = train(Xt,yt,False)        \n",
    "            bench_train+=time.time()-t1\n",
    "        ls=ls+l\n",
    "        nrls=nrls+1\n",
    "        cur_loss=ls/nrls\n",
    "        if time.time()-last_snapshot > snapshot_interval_sec:\n",
    "            t1=time.time()\n",
    "            nrls=0\n",
    "            ls=0\n",
    "            last_snapshot=time.time()\n",
    "            print(f\"Epoch {e+1} Loss: {cur_loss} Precision: {pr}\")\n",
    "            save_checkpoint(e,cur_loss,pr, best_pr)\n",
    "            # if use_cuda:\n",
    "            #     print(f\"Cuda memory allocated: {torch.cuda.memory_allocated()} max_alloc: {torch.cuda.max_memory_allocated()} cached: {torch.cuda.memory_cached()} max_cached: {torch.cuda.max_memory_cached()}\")\n",
    "            hist={\"epoch\": e, \"loss\": cur_loss, \"precision\": pr, \"timestamp\": time.time()-start_time}\n",
    "            history.append(hist)\n",
    "            save_history(history)\n",
    "            bench_snapshot+=time.time()-t1\n",
    "\n",
    "            if bench_all > 0:\n",
    "                bd=bench_data/bench_all*100.0\n",
    "                bt=(bench_train+bench_train_withprec)/bench_all*100.0\n",
    "                bs=bench_sample/bench_all*100.0\n",
    "                bss=bench_snapshot/bench_all*100.0\n",
    "                bo=(bench_all-bench_data-bench_train-bench_train_withprec-bench_sample-bench_snapshot)/bench_all*100.0\n",
    "                print(f\"Benchmarks: data-loading: {bd:.2f}%, training: {bt:.2f}%, sample gen: {bs:.2f}%, snapshots: {bss:.2f}%, overhead: {bo:.2f}%\")\n",
    "\n",
    "        if time.time()-last_sample > sample_interval_sec:\n",
    "            t1=time.time()\n",
    "            last_sample=time.time()\n",
    "            for temperature in [0.6, 0.8, 1.0]:\n",
    "                print(f\"Temperature {temperature}:\")\n",
    "                tgen=poet.generate(700,\". \", temperature=temperature)\n",
    "                textlib.source_highlight(tgen,minQuoteSize=10,dark_mode=use_dark_mode,display_ref_anchor=False)\n",
    "            bench_sample+=time.time()-t1\n",
    "        t1=time.time()\n",
    "    bench_all+=time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Xt, yt in data_loader:\n",
    "    print(Xt,yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dsSPjPsUlpY"
   },
   "source": [
    "# 4. Text generation\n",
    "\n",
    "## 4.1 Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-vHDcHupruq"
   },
   "outputs": [],
   "source": [
    "load_checkpoint(filename=\"model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oflaWxltsJd6"
   },
   "outputs": [],
   "source": [
    "print(\"Sample text:\")\n",
    "print(\"\")\n",
    "for temperature in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6]:\n",
    "    tgen=poet.generate(1000,\"\\n\\n\", temperature=temperature)\n",
    "    print(f\"================Temperature: {temperature}==============\")\n",
    "    detectPlagiarism(tgen, textlib, display_ref_anchor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdpCtjvfUlpZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10, display_ref_anchor=True):\n",
    "    textlibrary.source_highlight(generatedtext, minQuoteSize=minQuoteLength,dark_mode=use_dark_mode, display_ref_anchor=display_ref_anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjCd8CHLUlpd"
   },
   "source": [
    "## 4.2 Dialog with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfiL1_64Ulpe"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "def doDialog():\n",
    "    temperature = 0.8  # 0.1 (free-style chaos) - >1.0 (rigid, frozen)\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
    "    # maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    # minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    \n",
    "    print(\"Please enter some dialog.\")\n",
    "    print(\"The net will answer according to your input.\")\n",
    "    print(\"'bye' for end,\")\n",
    "    print(\"'reset' to reset the conversation context,\")\n",
    "    print(\"'temperature=<float>' [0.1(free, chaotic) - >1.0(strict, frozen)]\")\n",
    "    print(\"    to change character of the dialog.\")\n",
    "    # print(\"    Current temperature={}.\".format(temperature))\n",
    "    print()\n",
    "    xso = None\n",
    "    bye = False\n",
    "    last_ans=\"\"\n",
    "        \n",
    "    while not bye:\n",
    "        print(\"> \", end=\"\")\n",
    "        prompt = input()\n",
    "        if prompt == 'bye':\n",
    "            bye = True\n",
    "            print(\"Good bye!\")\n",
    "            continue\n",
    "        if prompt.find(\"temperature\")>=0 and prompt.find(\"=\") > prompt.find(\"temperature\"):\n",
    "            temperature=float(prompt[prompt.find('=')+1:])\n",
    "            print(f\"Temperature set to {temperature}\")\n",
    "            continue\n",
    "        for attempts in range(1,3):\n",
    "            tgen=poet.generate(1000,last_ans+\"\\n\\n\"+prompt,temperature=temperature)\n",
    "            i=tgen.find(endPrompt)\n",
    "            tgen=tgen.replace(\"Mr.\", \"Mr\")\n",
    "            tgen=tgen.replace(\"Mrs.\", \"Mrs\")\n",
    "            tgen=tgen.replace(\"\\n\",\" \")\n",
    "            tgen=tgen.replace(\"  \",\" \")\n",
    "            i2=tgen[i+1:].find(endPrompt)+i\n",
    "            i3=tgen[i2+1:].find(endPrompt)+i2\n",
    "            tgen=tgen[i+1:i3+2]\n",
    "            if len(tgen)>10:\n",
    "                break\n",
    "        last_ans=tgen\n",
    "        textlib.source_highlight(tgen, minQuoteSize=10,dark_mode=use_dark_mode,display_ref_anchor=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mGGsuFRUlpi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eON9sYdz_1lh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of torch_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
