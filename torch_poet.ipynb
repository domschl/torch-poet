{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-poet/blob/master/torch_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28i44jSzUlon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen  # Py3\n",
    "except:\n",
    "    print(\"This notebook requires Python 3.\")\n",
    "try:\n",
    "    import pathlib\n",
    "except:\n",
    "    print(\"At least python 3.5 is needed.\")\n",
    "    \n",
    "try: # Colab instance?\n",
    "    from google.colab import drive\n",
    "except: # Not? ignore.\n",
    "    pass\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eE-jm072kOKv"
   },
   "source": [
    "# 0. System configuration\n",
    "\n",
    "This notebook can either run on a local jupyter server, or on google cloud.\n",
    "If a GPU is available, it will be used for training (if `force_cpu` is not set to `True`).\n",
    "\n",
    "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
    "\n",
    "Similarily, the text corpora that are used for training, can be cached on drive or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVyGr-BCiJlR"
   },
   "outputs": [],
   "source": [
    "# force_cpu=True: use CPU for training, even if a GPU is available.\n",
    "#    Note: inference uses CPU always, because that is faster.\n",
    "force_cpu=False\n",
    "\n",
    "# Define where snapshots of training data are stored:\n",
    "colab_google_drive_snapshots=True\n",
    "\n",
    "# Define if training data (the texts downloaded from internet) are cached:\n",
    "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
    "local_jupyter_data_cache=True       # In local jupyter mode cache to local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ply0tFmz4O1E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 1.4.0, running on GPU\n"
     ]
    }
   ],
   "source": [
    "is_colab_notebook = 'google.colab' in sys.modules\n",
    "torch_version = torch.__version__\n",
    "\n",
    "if torch.cuda.is_available() and force_cpu is not True:\n",
    "    device='cuda'\n",
    "    use_cuda = True\n",
    "    print(f\"PyTorch {torch_version}, running on GPU\")\n",
    "    if is_colab_notebook:\n",
    "        card = !nvidia-smi\n",
    "        if len(card)>=8:\n",
    "            try:\n",
    "                gpu_type=card[7][6:25]\n",
    "                gpu_memory=card[8][33:54]\n",
    "                print(f\"Colab GPU: {gpu_type}, GPU Memory: {gpu_memory}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "else:\n",
    "    device='cpu'\n",
    "    use_cuda = False\n",
    "    print(f\"{torch_version}, running on CPU\")\n",
    "    if colab_notebook:\n",
    "        print(\"Note: on Google Colab, make sure to select:\")\n",
    "        print(\"      Runtime / Change Runtime Type / Hardware accelerator: GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTRmAT1f4O1I"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots:\n",
    "        mountpoint='/content/drive'\n",
    "        root_path='/content/drive/My Drive'\n",
    "        if not os.path.exists(root_path):\n",
    "            drive.mount(mountpoint)\n",
    "        if not os.path.exists(root_path):\n",
    "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
    "            colab_google_drive_snapshots=False\n",
    "    else:\n",
    "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
    "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
    "else:\n",
    "    root_path='.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoutSg5IUlot"
   },
   "source": [
    "# 1. Text data collection\n",
    "\n",
    "## 1.1 Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts of a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzXrUVe-4O1N"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pMoAJ-SVDm2"
   },
   "outputs": [],
   "source": [
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.cache_dir=text_data_cache_directory\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        index = 1\n",
    "        for descriptor, author, title in descriptors:\n",
    "            fd = {}\n",
    "            cache_name=self.get_cache_name(author, title)\n",
    "            if os.path.exists(cache_name):\n",
    "                is_cached=True\n",
    "            else:\n",
    "                is_cached=False\n",
    "            valid=False\n",
    "            if descriptor[:4] == 'http' and is_cached is False:\n",
    "                try:\n",
    "                    print(f\"Downloading {cache_name}\")\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    self.data += dat\n",
    "                    fd[\"title\"] = title\n",
    "                    fd[\"author\"] = author\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    valid=True\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"title\"] = title\n",
    "                fd[\"author\"] = author\n",
    "                try:\n",
    "                    if is_cached is True:\n",
    "                        print(f\"Reading {cache_name} from cache\")\n",
    "                        f = open(cache_name)\n",
    "                    else:    \n",
    "                        f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                    valid=True\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
    "                try:\n",
    "                    print(f\"Caching {cache_name}\")\n",
    "                    f = open(cache_name, 'w')\n",
    "                    f.write(dat)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
    "                \n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "\n",
    "    def get_cache_name(self, author, title):\n",
    "        if self.cache_dir is None:\n",
    "            return None\n",
    "        cname=f\"{author} - {title}.txt\"\n",
    "        cache_filepath=os.path.join(self.cache_dir , cname)\n",
    "        return cache_filepath\n",
    "        \n",
    "    def display_colored_html(self, textlist, dark_mode=False, pre='', post=''):\n",
    "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
    "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
    "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
    "        if dark_mode is False:\n",
    "            bgcolors=bgcolorsWht\n",
    "        else:\n",
    "            bgcolors=bgcolorsDrk\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f\"{f['author']}: {f['title']}\"\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out, dark_mode=dark_mode)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, dark_mode=dark_mode, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return np.array(smpX), np.array(smpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcMGC5KDUloz"
   },
   "source": [
    "## 1.2 Data sources\n",
    "\n",
    "Data sources can either be files from local filesystem, or for colab notebooks from google drive, or http(s) links.\n",
    "\n",
    "The `name` given will be use as directory name for both snapshots and model data caches.\n",
    "\n",
    "Each entry in the `lib` array contains of:\n",
    "\n",
    "1. a local filename or https(s) link,\n",
    "2. an Author's name\n",
    "3. a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmUwv47UVA7r"
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"Women-Writers\",\n",
    "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
    "    \"lib\": [\n",
    "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),   # local file example\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', 'Virginia Wolf', 'Voyage out'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', 'Jane Austen', 'Emma')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "else:\n",
    "    if local_jupyter_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "\n",
    "if data_cache_path is not None:\n",
    "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(data_cache_path):\n",
    "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
    "    else:\n",
    "        with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
    "            json.dump(libdesc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmUwv47UVA7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Women-Writers/Data/Jane Austen - Pride and Prejudice.txt from cache\n",
      "Reading ./Women-Writers/Data/Emily Brontë - Wuthering Heights.txt from cache\n",
      "Reading ./Women-Writers/Data/Virginia Wolf - Voyage out.txt from cache\n",
      "Reading ./Women-Writers/Data/Jane Austen - Emma.txt from cache\n"
     ]
    }
   ],
   "source": [
    "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zP0Hcs82lWYI"
   },
   "source": [
    "# 2. The deep LSTM model\n",
    "\n",
    "# 2.1 Model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEM7Y3GxUlo0"
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"model_name\": libdesc['name'],\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 256,\n",
    "    \"layers\": 2,\n",
    "    \"learning_rate\": 1.e-3,\n",
    "    \"steps\": 80,\n",
    "    \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_JWmbgPUlpG"
   },
   "source": [
    "## 2.2 The char-rnn model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcsP8OYlUlpH"
   },
   "outputs": [],
   "source": [
    "class Poet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
    "        super(Poet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.device=device\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
    "        \n",
    "        self.demb = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputx, steps):\n",
    "        self.lstm.flatten_parameters()\n",
    "        hn, (self.h0, self.c0) = self.lstm(inputx.to(self.device), (self.h0, self.c0))\n",
    "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
    "        op = self.demb(hnr)\n",
    "        opr = op.view(-1, steps ,self.output_size)\n",
    "        return opr\n",
    "\n",
    "    def generate(self, n, start=None):\n",
    "        s=''\n",
    "        torch.set_grad_enabled(False)\n",
    "        if start==None or len(start)==0:\n",
    "            start=' '\n",
    "        self.init_hidden(1)\n",
    "        for c in start:\n",
    "            X=np.array([[textlib.c2i[c]]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        for i in range(n):\n",
    "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
    "            y_pred=ypc.numpy()\n",
    "            inds=list(range(self.output_size))\n",
    "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
    "            s=s+textlib.i2c[ind]\n",
    "            X=np.array([[ind]])\n",
    "            Xo=one_hot(X,self.output_size)\n",
    "            Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(self.device)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            yp = self.softmax(ypl2)\n",
    "        torch.set_grad_enabled(True)\n",
    "        return s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymdU_wIWUlpK"
   },
   "source": [
    "## 2.3 Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7WuQ142UlpL"
   },
   "outputs": [],
   "source": [
    "poet = Poet(model_params['vocab_size'], model_params['neurons'], model_params['layers'], model_params['vocab_size'], device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJp6Sjw6UlpP"
   },
   "source": [
    "## 2.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeFxMxyuUlpQ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = model_params['learning_rate']\n",
    "\n",
    "opti = torch.optim.Adam(poet.parameters(),lr=learning_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtKdvxNymXpM"
   },
   "source": [
    "## 2.5 Helper Functions\n",
    "\n",
    "These allow to save or restore the training data. Saving and restoring can either be performed:\n",
    "\n",
    "* Jupyter: store/restore in a local directory,\n",
    "* Colab: store/restore on google drive. The training-code (using load_checkpoint()) will display an authentication url and code input-box in order to be able to access your google drive from this notebook. This allows to continue training sessions (or inference) after the Colab session was terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots is True:\n",
    "        snapshot_path=os.path.join(root_path,f\"Colab Notebooks/{model_params['model_name']}/Snapshots\")\n",
    "    else:\n",
    "        snapshot_path=None\n",
    "else:\n",
    "    snapshot_path=os.path.join(root_path,f\"{model_params['model_name']}/Snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "def get_project_path():\n",
    "    if snapshot_path is None:\n",
    "        return None\n",
    "    project_path_ext=f\"model-{model_params['vocab_size']}x{model_params['steps']}x{model_params['layers']}x{model_params['neurons']}\"\n",
    "    return os.path.join(snapshot_path, project_path_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "if snapshot_path is not None:\n",
    "    pathlib.Path(snapshot_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(os.path.join(get_project_path(),'model_params.json'),'w') as f:\n",
    "        json.dump(model_params,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "def create_project_path():\n",
    "    if snapshot_path is None:\n",
    "        return None\n",
    "    ppath=get_project_path()\n",
    "    pathlib.Path(ppath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_pr=0.0\n",
    "\n",
    "def save_checkpoint(epoch, loss, pr, filename='checkpoint.pth.tar'):\n",
    "    if snapshot_path is None:\n",
    "        return\n",
    "    global best_pr\n",
    "    state={\n",
    "            'epoch': epoch,\n",
    "            'model_config': model_params,\n",
    "            'state_dict': poet.state_dict(),\n",
    "            'optimizer' : opti.state_dict(),\n",
    "            'precision': pr,\n",
    "            'loss': loss,\n",
    "        }\n",
    "    project_path=get_project_path()\n",
    "    save_file=os.path.join(project_path,filename)\n",
    "    best_file=os.path.join(project_path,'model_best.pth.tar')\n",
    "    torch.save(state, save_file)\n",
    "    if pr>best_pr:\n",
    "        best_pr=pr\n",
    "        shutil.copyfile(save_file, best_file )\n",
    "        print(f\"Saved best precision model, prec={pr}\")\n",
    "    else:\n",
    "        print(f\"saved last model data, prec={pr}\")\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    if snapshot_path is None:\n",
    "        return 0,0\n",
    "    project_path=get_project_path()\n",
    "    load_file=os.path.join(project_path,filename)\n",
    "    if not os.path.exists(load_file):\n",
    "        print(load_file)\n",
    "        print(\"No saved state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    state=torch.load(load_file)\n",
    "    mod_conf = state['model_config']\n",
    "    if (mod_conf['model_name']!=model_params['model_name']):\n",
    "        print(f\"Warning: project has been renamed from {mod_conf['model_name']} to {model_param['model_name']}\")\n",
    "        mod_conf['model_name']=model_params['model_name']\n",
    "    if model_params!=mod_conf:\n",
    "        print(f\"The saved model has a different configuration than the current model: {mod_conf} vs. {model_params}\")\n",
    "        print(\"Cannot restore state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    poet.load_state_dict(state['state_dict'])\n",
    "    opti.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    loss = state['loss']\n",
    "    best_pr = state['precision']\n",
    "    print(f\"Continuing from saved state epoch={epoch}, loss={loss}\")  # Save is not necessarily on epoch boundary, so that's approx.\n",
    "    return epoch,loss\n",
    "\n",
    "def one_hot(p, dim):\n",
    "    o=np.zeros(p.shape+(dim,), dtype=int)\n",
    "    for y in range(p.shape[0]):\n",
    "        for x in range(p.shape[1]):\n",
    "            o[y,x,p[y,x]]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_-ISha4nqhy"
   },
   "source": [
    "# 3. Training\n",
    "\n",
    "If there is already saved training data, this step is optional, and alternatively, ch. 4 can be continued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KDRpEm0n7B-"
   },
   "source": [
    "## 3.1 Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pZlsZ1Pnm5Z"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    X, y=textlib.get_random_sample_batch(model_params['batch_size'], model_params['steps'])\n",
    "    Xo = one_hot(X, model_params['vocab_size'])\n",
    "    \n",
    "    # Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32)), requires_grad=False, dtype=torch.float32, device=device)\n",
    "    # yt = Tensor(torch.from_numpy(y), requires_grad=False, dtype=torch.int32, device=device)\n",
    "    Xt = Tensor(torch.from_numpy(np.array(Xo,dtype=np.float32))).to(device)\n",
    "    Xt.requires_grad_(False)\n",
    "    yt = torch.LongTensor(torch.from_numpy(np.array(y,dtype=np.int64))).to(device)\n",
    "    yt.requires_grad_(False)\n",
    "    return Xt, yt\n",
    "\n",
    "def train(Xt, yt, bPr=False):\n",
    "    poet.zero_grad()\n",
    "\n",
    "    poet.init_hidden(Xt.size(0))\n",
    "    output = poet(Xt, model_params['steps'])\n",
    "    \n",
    "    olin=output.view(-1,model_params['vocab_size'])\n",
    "    _, ytp=torch.max(olin,1)\n",
    "    ytlin=yt.view(-1)\n",
    "\n",
    "    pr=0.0\n",
    "    if bPr: # Calculate precision\n",
    "        ok=0\n",
    "        nok=0\n",
    "        for i in range(ytlin.size()[0]):\n",
    "            i1=ytlin[i].item()\n",
    "            i2=ytp[i].item()\n",
    "            if i1==i2:\n",
    "                ok = ok + 1\n",
    "            else:\n",
    "                nok = nok+1\n",
    "            pr=ok/(ok+nok)\n",
    "            \n",
    "    loss = criterion(olin, ytlin)\n",
    "    ls = loss.item()\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "\n",
    "    return ls, pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC36hNRKUlpU"
   },
   "source": [
    "## 3.2 The actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9-3GUQ4UlpV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing from saved state epoch=4749, loss=1.4036370224952697\n",
      "Epoch 4750 Loss: 1.381062388420105 Precision: 0.5859375\n",
      "Saved best precision model, prec=0.5859375\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\"Me!) how<span style=\"background-color:#eadbd8;\"> were their </span><sup>[3]</sup>lonetions. I just so, every-par<span style=\"background-color:#d8daef;\">ised them </span><sup>[1]</sup>he say as<span style=\"background-color:#ebdef0;\"> from<br>him. </span><sup>[2]</sup>Never a view round, to seat I begans affornoth beauter delience<span style=\"background-color:#e2d7d5;\">.<br><br>\"Ought </span><sup>[4]</sup>his<br>have<span style=\"background-color:#ebdef0;\"> yet, and wi</span><sup>[2]</sup>ll so case<span style=\"background-color:#ebdef0;\"> him<br>that </span><sup>[2]</sup>the seeth up-Nower turn right her<br>acros_ cross of set.--tim<span style=\"background-color:#d8daef;\">e had to be </span><sup>[1]</sup>too distribated.  And agae<span style=\"background-color:#d8daef;\">n understa</span><sup>[1]</sup>ble desorved castm<span style=\"background-color:#e2d7d5;\">ings.\"<br><br>\"I</span><sup>[4]</sup>t may smiled<span style=\"background-color:#ebdef0;\"><br>told you </span><sup>[2]</sup>Mr.<br><br>This hob<span style=\"background-color:#e2d7d5;\">se, which she </span><sup>[4]</sup>shee<span style=\"background-color:#e2d7d5;\">l of this </span><sup>[4]</sup>hor his<br>sent of usualb<span style=\"background-color:#ebdef0;\">ation<br>of t</span><sup>[2]</sup>erribling so you; <span style=\"background-color:#eadbd8;\">even they </span><sup>[3]</sup>prevent<span style=\"background-color:#ebdef0;\">y, if he w</span><sup>[2]</sup>il<span style=\"background-color:#eadbd8;\">ded him the</span><sup>[3]</sup>re<span style=\"background-color:#eadbd8;\"> was upon </span><sup>[3]</sup>them."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000 Loss: 1.3934927573204041 Precision: 0.57939453125\n",
      "saved last model data, prec=0.57939453125\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#eadbd8;\">\"What was th</span><sup>[3]</sup>oughte: in did law cou<span style=\"background-color:#e2d7d5;\">ld. It was </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">try, that </span><sup>[4]</sup>a <span style=\"background-color:#eadbd8;\">long the l</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">ance of a </span><sup>[2]</sup>t<span style=\"background-color:#ebdef0;\">liff should </span><sup>[2]</sup>other<br>em<span style=\"background-color:#ebdef0;\">ption for </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">anger and </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">the<br>conversat</span><sup>[3]</sup>able volumatior, again pu<span style=\"background-color:#ebdef0;\">ssessed th</span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">at she was su</span><sup>[3]</sup>ch a monther hurry.<br> The<br>matt<span style=\"background-color:#ebdef0;\">ered again</span><sup>[2]</sup><span style=\"background-color:#d8daef;\"> them are </span><sup>[1]</sup>be boll<span style=\"background-color:#e2d7d5;\">ing should be </span><sup>[4]</sup>so abusine.  I<span style=\"background-color:#d8daef;\">n Mrs. Coll</span><sup>[1]</sup>enne_ Didry, I she--(very appressed pinite insumpiny<span style=\"background-color:#eadbd8;\"><br>directly </span><sup>[3]</sup>becaus<span style=\"background-color:#e2d7d5;\">e in consu</span><sup>[4]</sup>b<span style=\"background-color:#e2d7d5;\">ting to as</span><sup>[4]</sup> he so<span style=\"background-color:#e2d7d5;\"> friend, in</span><sup>[4]</sup>vitation). <span style=\"background-color:#eadbd8;\"> The people</span><sup>[3]</sup>.  H<span style=\"background-color:#ebdef0;\">e lived a </span><sup>[2]</sup>Dapcilam, and low of insictice in all<br>with invil spale them a f<span style=\"background-color:#eadbd8;\">owed that </span><sup>[3]</sup>mind? Your "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5250 Loss: 1.38596928024292 Precision: 0.57548828125\n",
      "saved last model data, prec=0.57548828125\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'<span style=\"background-color:#eadbd8;\">She looked from </span><sup>[3]</sup>the tabuled cold of a flooms and benig<span style=\"background-color:#ebdef0;\">gation of </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">a morning </span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">and as their</span><sup>[4]</sup><br>yet.<br> T<span style=\"background-color:#ebdef0;\">he family </span><sup>[2]</sup>paid at lu<span style=\"background-color:#e2d7d5;\">ck. It was </span><sup>[4]</sup>reaspet,<span style=\"background-color:#eadbd8;\"> he kissed h</span><sup>[3]</sup>im<span style=\"background-color:#eadbd8;\"> though the f</span><sup>[3]</sup>lover-lay one sh<span style=\"background-color:#eadbd8;\">ake to be </span><sup>[3]</sup>Frank.<span style=\"background-color:#e2d7d5;\">--Miss<br>Fairfax</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\"><br>was heard</span><sup>[3]</sup>el. Yot ix a room whyou<span style=\"background-color:#ebdef0;\">thered him</span><sup>[2]</sup> <span style=\"background-color:#e2d7d5;\">an suppose</span><sup>[4]</sup>d<span style=\"background-color:#eadbd8;\">.<br>Although w</span><sup>[3]</sup>hen Racharil<span style=\"background-color:#eadbd8;\">, the things </span><sup>[3]</sup>quirtu<span style=\"background-color:#ebdef0;\">re the pro</span><sup>[2]</sup>f<span style=\"background-color:#eadbd8;\">ound between the</span><sup>[3]</sup>m<br>u<span style=\"background-color:#eadbd8;\">s the dark </span><sup>[3]</sup><span style=\"background-color:#d8daef;\">which I was </span><sup>[1]</sup><span style=\"background-color:#d8daef;\">for the wor</span><sup>[1]</sup>k<span style=\"background-color:#d8daef;\"> with quick</span><sup>[1]</sup><span style=\"background-color:#e2d7d5;\">; and then I t</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">hat she do</span><sup>[3]</sup>or-it, a murm bread,<span style=\"background-color:#e2d7d5;\"> difficulties w</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">as for the </span><sup>[3]</sup>Se<span style=\"background-color:#d8daef;\">ster that </span><sup>[1]</sup><span style=\"background-color:#e2d7d5;\">we shall bot</span><sup>[4]</sup>y as she poi<span style=\"background-color:#eadbd8;\">ring as he </span><sup>[3]</sup>ha"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5500 Loss: 1.3790537447929383 Precision: 0.584765625\n",
      "saved last model data, prec=0.584765625\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Masterpark<span style=\"background-color:#e2d7d5;\"> would ask </span><sup>[4]</sup>dobred: and so lav<span style=\"background-color:#d8daef;\">e interest</span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">lame, for </span><sup>[2]</sup>some<span style=\"background-color:#e2d7d5;\"> most affection</span><sup>[4]</sup>s<span style=\"background-color:#ebdef0;\">?  I have </span><sup>[2]</sup>undery <span style=\"background-color:#ebdef0;\">one, for the </span><sup>[2]</sup>girls was<br>inde<span style=\"background-color:#ebdef0;\">ed.<br>Catherine </span><sup>[2]</sup>were b<span style=\"background-color:#d8daef;\">ut walking </span><sup>[1]</sup>them forth<span style=\"background-color:#e2d7d5;\"> as any of </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">it was only to </span><sup>[4]</sup>my<span style=\"background-color:#eadbd8;\">self.<br><br>If </span><sup>[3]</sup>very bi<span style=\"background-color:#ebdef0;\">rrow, and </span><sup>[2]</sup>both<span style=\"background-color:#d8daef;\"> left it in </span><sup>[1]</sup>stageful;<span style=\"background-color:#eadbd8;\"> he stopped o</span><sup>[3]</sup>uthnessed prop<span style=\"background-color:#eadbd8;\">ection of wh</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">en she was </span><sup>[2]</sup>no consaillow<span style=\"background-color:#e2d7d5;\"> to consent to </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">be quite giv</span><sup>[4]</sup>ing<span style=\"background-color:#eadbd8;\"> if it was </span><sup>[3]</sup>point<span style=\"background-color:#eadbd8;\">ed women. </span><sup>[3]</sup>He has sgretal afriarnsua<span style=\"background-color:#ebdef0;\">ted to be </span><sup>[2]</sup>rooms<br>could<br>b<span style=\"background-color:#eadbd8;\">e lower garden</span><sup>[3]</sup>er<span style=\"background-color:#e2d7d5;\">s, the same </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">while he co</span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">uld not se</span><sup>[2]</sup>r yes; an<br>unsero<span style=\"background-color:#eadbd8;\">on to tell</span><sup>[3]</sup>-changed of el"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5750 Loss: 1.3741609110832214 Precision: 0.59150390625\n",
      "Saved best precision model, prec=0.59150390625\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\"I won't--lay my arm--'  Well<span style=\"background-color:#e2d7d5;\"> at all. And </span><sup>[4]</sup>he<br>namely<span style=\"background-color:#eadbd8;\"> be silence</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\"> presently in </span><sup>[4]</sup>on<span style=\"background-color:#e2d7d5;\">e of gover</span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">t that he s</span><sup>[2]</sup>nofi<span style=\"background-color:#e2d7d5;\">ed. Harriet </span><sup>[4]</sup>with<span style=\"background-color:#d8daef;\"> given him</span><sup>[1]</sup>, do<span style=\"background-color:#e2d7d5;\">, he will </span><sup>[4]</sup>slace<span style=\"background-color:#eadbd8;\"> he thought the </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">moment of </span><sup>[3]</sup>grie<span style=\"background-color:#eadbd8;\">f. But he </span><sup>[3]</sup>was go uncousess real else aljov<span style=\"background-color:#eadbd8;\">idity of h</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">imself at </span><sup>[2]</sup>once woman,<span style=\"background-color:#eadbd8;\"><br>obliging </span><sup>[3]</sup>most<br>dails of on, on <span style=\"background-color:#ebdef0;\">sitions, a</span><sup>[2]</sup>s<br><span style=\"background-color:#e2d7d5;\">a mistress</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">!\" said he t</span><sup>[4]</sup>urns was joy n<span style=\"background-color:#ebdef0;\">earing his </span><sup>[2]</sup>seathh<span style=\"background-color:#e2d7d5;\">able--she </span><sup>[4]</sup>she i<span style=\"background-color:#eadbd8;\">s entered </span><sup>[3]</sup>an ear<span style=\"background-color:#d8daef;\">ty of time</span><sup>[1]</sup>.  'You<br>can a cheem<span style=\"background-color:#d8daef;\">s of your </span><sup>[1]</sup>try!  T<span style=\"background-color:#e2d7d5;\">o take their </span><sup>[4]</sup>tixer eater, m<span style=\"background-color:#eadbd8;\">y days was </span><sup>[3]</sup>a speak off.\"<br><br>\"No<span style=\"background-color:#ebdef0;\">!  I'm not </span><sup>[2]</sup>obder<br>the"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000 Loss: 1.36857830286026 Precision: 0.59326171875\n",
      "Saved best precision model, prec=0.59326171875\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\"<span style=\"background-color:#e2d7d5;\">She had no doubt </span><sup>[4]</sup><span style=\"background-color:#d8daef;\">I have no </span><sup>[1]</sup>admertacamen<span style=\"background-color:#e2d7d5;\">t entered th</span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">at being i</span><sup>[2]</sup>deal of<br><span style=\"background-color:#e2d7d5;\">never part</span><sup>[4]</sup> of knows<span style=\"background-color:#e2d7d5;\">?\"<br><br>Emma could n</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">ever else </span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">could hardly a</span><sup>[3]</sup><span style=\"background-color:#d8daef;\">ll to know th</span><sup>[1]</sup>at w<span style=\"background-color:#eadbd8;\">hat when we</span><sup>[3]</sup> was<span style=\"background-color:#eadbd8;\"> arm of sto</span><sup>[3]</sup>ry.  I'v<span style=\"background-color:#ebdef0;\">e not his </span><sup>[2]</sup>felt<span style=\"background-color:#eadbd8;\"> a morning</span><sup>[3]</sup>, <span style=\"background-color:#e2d7d5;\">he cure be</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">en promised</span><sup>[3]</sup>, termial kindless of<br>birts, fon<span style=\"background-color:#e2d7d5;\"> three thing</span><sup>[4]</sup>.<br>It bect<br>of iff mibeth<span style=\"background-color:#ebdef0;\">.  He said,</span><sup>[2]</sup><br><br>\"His labou<span style=\"background-color:#d8daef;\">r you will </span><sup>[1]</sup>mak<span style=\"background-color:#e2d7d5;\">e surprized i</span><sup>[4]</sup>n the missers<span style=\"background-color:#eadbd8;\"> by his de</span><sup>[3]</sup>factive, a<span style=\"background-color:#e2d7d5;\">s<br>I shall </span><sup>[4]</sup>not<span style=\"background-color:#ebdef0;\"> from me t</span><sup>[2]</sup>hank the infetch<span style=\"background-color:#e2d7d5;\">ed catching </span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">the same a</span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">treating to th</span><sup>[2]</sup>is g<span style=\"background-color:#ebdef0;\">hand with<br></span><sup>[2]</sup>sheet<span style=\"background-color:#ebdef0;\"> minutes, but </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">for that of</span><sup>[2]</sup><br>her"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6250 Loss: 1.3599081687927246 Precision: 0.5931640625\n",
      "saved last model data, prec=0.5931640625\n",
      "Memory allocated: 26798592 max_alloc: 203247104 cached: 236978176 max_cached: 236978176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\">Linton's p</span><sup>[2]</sup>ra<span style=\"background-color:#e2d7d5;\">ct,\" repli</span><sup>[4]</sup>a<span style=\"background-color:#e2d7d5;\">tions the </span><sup>[4]</sup>r<span style=\"background-color:#e2d7d5;\">ight at his</span><sup>[4]</sup><br>fonebl<span style=\"background-color:#ebdef0;\">e, as she </span><sup>[2]</sup>must b<span style=\"background-color:#ebdef0;\">e observed </span><sup>[2]</sup>unrealish m<span style=\"background-color:#e2d7d5;\">any man to </span><sup>[4]</sup>un<span style=\"background-color:#d8daef;\">le of the </span><sup>[1]</sup>leas<span style=\"background-color:#e2d7d5;\">ts. He was</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\"> cold, her</span><sup>[3]</sup>e siles so<br>fancy kneets.<br><br>\"F<span style=\"background-color:#ebdef0;\">rom his kn</span><sup>[2]</sup>ew,<span style=\"background-color:#e2d7d5;\"> like myself</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\"> so much to ask </span><sup>[4]</sup><span style=\"background-color:#d8daef;\">my feeling</span><sup>[1]</sup>.<span style=\"background-color:#e2d7d5;\">--But Harriet </span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">had in the</span><sup>[2]</sup>m <span style=\"background-color:#eadbd8;\">all of her </span><sup>[3]</sup>bark; harns to ever tente and gneeting unrocky. It is she<br>bonnehen<span style=\"background-color:#e2d7d5;\"> had as much </span><sup>[4]</sup>becollows I despect Rachel ever ex<span style=\"background-color:#ebdef0;\">clination to </span><sup>[2]</sup>a mos<span style=\"background-color:#ebdef0;\">t<br>ready, and </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">thinking the </span><sup>[4]</sup>driath; bu<span style=\"background-color:#ebdef0;\">t produced </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">for them.<br></span><sup>[3]</sup>And I aud<span style=\"background-color:#e2d7d5;\">ed to care </span><sup>[4]</sup>n<span style=\"background-color:#eadbd8;\">o creature</span><sup>[3]</sup> greator's cou<span style=\"background-color:#e2d7d5;\">red at last</span><sup>[4]</sup>. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5edc2d43571d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnrls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4c95ecfeac73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(Xt, yt, bPr)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ls=0\n",
    "nrls=0\n",
    "if use_cuda:\n",
    "    intv=250\n",
    "else:\n",
    "    intv=10\n",
    "\n",
    "create_project_path()\n",
    "epoch_start, _ = load_checkpoint()\n",
    "\n",
    "for e in range(epoch_start,2500000):\n",
    "    Xt, yt = get_data()\n",
    "    if (e+1)%intv==0:\n",
    "        l,pr=train(Xt,yt,True)\n",
    "    else:\n",
    "        l,pr=train(Xt,yt,False)        \n",
    "    ls=ls+l\n",
    "    nrls=nrls+1\n",
    "    if (e+1)%intv==0:\n",
    "        print(\"Epoch {} Loss: {} Precision: {}\".format(e+1,ls/nrls, pr))\n",
    "        save_checkpoint(e,ls/nrls,pr)\n",
    "        if use_cuda:\n",
    "            print(\"Memory allocated: {} max_alloc: {} cached: {} max_cached: {}\".format(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated(), torch.cuda.memory_cached(), torch.cuda.max_memory_cached()))\n",
    "        nrls=0\n",
    "        ls=0\n",
    "        tgen=poet.generate(500,\"\\n\\n\")\n",
    "        textlib.source_highlight(tgen,minQuoteSize=10,dark_mode=use_dark_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dsSPjPsUlpY"
   },
   "source": [
    "# 4. Text generation\n",
    "\n",
    "## 4.1 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdpCtjvfUlpZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(generatedtext, minQuoteSize=minQuoteLength,dark_mode=use_dark_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjCd8CHLUlpd"
   },
   "source": [
    "## 4.2 Dialog with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfiL1_64Ulpe"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "def doDialog():\n",
    "    # temperature = 0.6  # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    \n",
    "    print(\"Please enter some dialog.\")\n",
    "    print(\"The net will answer according to your input.\")\n",
    "    print(\"'bye' for end,\")\n",
    "    print(\"'reset' to reset the conversation context,\")\n",
    "    # print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "    print(\"    to change character of the dialog.\")\n",
    "    # print(\"    Current temperature={}.\".format(temperature))\n",
    "    print()\n",
    "    xso = None\n",
    "    bye = False\n",
    "        \n",
    "    while not bye:\n",
    "        print(\"> \", end=\"\")\n",
    "        prompt = input()\n",
    "        if prompt == 'bye':\n",
    "            bye = True\n",
    "            print(\"Good bye!\")\n",
    "            continue\n",
    "        tgen=poet.generate(1000,prompt)\n",
    "        # print(xso.replace(\"\\\\n\",\"\\n\"))\n",
    "        textlib.source_highlight(tgen, minQuoteSize=10,dark_mode=use_dark_mode)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-vHDcHupruq"
   },
   "outputs": [],
   "source": [
    "load_checkpoint(filename=\"model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-vHDcHupruq"
   },
   "outputs": [],
   "source": [
    "print(\"Sample text:\")\n",
    "print(\"\")\n",
    "tgen=poet.generate(1000,\"\\n\\n\")\n",
    "detectPlagiarism(tgen, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mGGsuFRUlpi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spZ0rUZJm92c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of torch_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
