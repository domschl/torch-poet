{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/torch-poet/blob/master/torch_poet_using_indie_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28i44jSzUlon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from enum import Enum\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "# import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cg7te10Bc0NG"
   },
   "outputs": [],
   "source": [
    "# Run this ONLY for TPU tests (this [currently=01/2022] DOWNGRADES torch for compatibility!):\n",
    "# See: https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=yUB12htcqU9W\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zzO8J6d0sa6"
   },
   "source": [
    "Please review [ml-indie-tools](https://github.com/domschl/ml-indie-tools), a collection machine learning tools that provides support for more environment indepent code. It will access your Google Drive when using with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3G2rh6itKVOV",
    "outputId": "9c801b36-70b7-4299-c948-ffb67be329e5"
   },
   "outputs": [],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQAINcHlKev3"
   },
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE-jm072kOKv"
   },
   "source": [
    "# 0. System configuration\n",
    "\n",
    "This notebook can either run on a local hardware, a jupyter server, or on Google Colab.\n",
    "\n",
    "This version of the notebook uses [ml-indie-tools](https://github.com/domschl/ml-indie-tools) to detect hardware, persistence handling, and access to training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "HVyGr-BCiJlR",
    "outputId": "b0d2b763-037b-4002-847d-4dc675f309c2"
   },
   "outputs": [],
   "source": [
    "ml_env=MLEnv(platform='pt')  # use PyTorch\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ply0tFmz4O1E",
    "outputId": "e0772c7d-18fa-44e6-9410-d3c5a43d03c7"
   },
   "outputs": [],
   "source": [
    "project_name='women_writers'\n",
    "model_name='lstm_v1'\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoutSg5IUlot"
   },
   "source": [
    "# 1. Text data collection\n",
    "\n",
    "**Important note:** the following `project_name` determines the root directory for training data and model snapshots, so it should be changed whenever datasets of model configurations are changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPkfXQbGjtp_"
   },
   "source": [
    "## 1.1 Project Gutenberg data source\n",
    "\n",
    "Search, filter, clean and download books from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz_LqFccjtqA"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AQCjVinN-hN"
   },
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrI3xSK7jtqL",
    "outputId": "fb573fda-7b5c-4749-9b63-b2cc124e5e8f"
   },
   "outputs": [],
   "source": [
    "# sample searches\n",
    "search_spec= {\"author\": [\"brontÃ«\",\"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
    "\n",
    "book_list=gd.search(search_spec)\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXqlcaNajtqb"
   },
   "source": [
    "## 1.2 Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts of a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTg4QFewsJdE",
    "outputId": "36c7df4c-1553-4ada-cceb-51a088d319d2"
   },
   "outputs": [],
   "source": [
    "td = Text_Dataset(book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnVzaGoRhytj"
   },
   "outputs": [],
   "source": [
    "class TextLibraryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_dataset, sample_length, torch_device, text_stepping=10):\n",
    "        self.device=torch_device\n",
    "        self.text_length=0\n",
    "        full_text=\"\"\n",
    "        for text in text_dataset.text_list:\n",
    "            if 'text' in text:\n",
    "                full_text += text['text']\n",
    "        text_encode = text_dataset.encode(full_text)\n",
    "        self.text_length = len(text_encode)\n",
    "        self.vocab_size = len(text_dataset.i2c)\n",
    "        self.text_stepping=text_stepping\n",
    "        self.sample_length=sample_length\n",
    "        self.records=int((self.text_length-sample_length-1)/text_stepping)\n",
    "        self.data=torch.LongTensor(text_encode).to(self.device)\n",
    "        del text_encode\n",
    "        del full_text\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.records\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if idx>=self.records:\n",
    "            return None\n",
    "        X=self.data[idx*self.text_stepping:idx*self.text_stepping+self.sample_length].to(self.device)\n",
    "        y=self.data[idx*self.text_stepping+1:idx*self.text_stepping+self.sample_length+1].to(self.device)\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP0Hcs82lWYI"
   },
   "source": [
    "# 2. The deep LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_JWmbgPUlpG"
   },
   "source": [
    "## 2.2 The char-rnn model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcsP8OYlUlpH"
   },
   "outputs": [],
   "source": [
    "class Poet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, device):\n",
    "        super(Poet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.device=device\n",
    "       \n",
    "        self.oh = torch.eye(input_size, device=self.device)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=0)\n",
    "        \n",
    "        self.demb = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # negative dims are a recent thing (as 2018-03), remove for old vers.\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "        self.c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "    def forward(self, inputx, steps):\n",
    "        lstm_input=self.oh[inputx]\n",
    "        self.lstm.flatten_parameters()\n",
    "        hn, (self.h0, self.c0) = self.lstm(lstm_input, (self.h0, self.c0))\n",
    "        hnr = hn.contiguous().view(-1,self.hidden_size)\n",
    "        op = self.demb(hnr)\n",
    "        opr = op.view(-1, steps ,self.output_size)\n",
    "        return opr\n",
    "\n",
    "    def generate(self, n, start=None, temperature=1.0):\n",
    "        s=''\n",
    "        torch.set_grad_enabled(False)\n",
    "        if start==None or len(start)==0:\n",
    "            start=' '\n",
    "        self.init_hidden(1)\n",
    "        for c in start:\n",
    "            Xt=torch.LongTensor([[td.c2i[c]]])\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            if temperature>0.0:\n",
    "                ypl2 = ypl2 / temperature\n",
    "            yp = self.softmax(ypl2)\n",
    "        for i in range(n):\n",
    "            ypc=Tensor.cpu(yp.detach()) # .cpu()\n",
    "            y_pred=ypc.numpy()\n",
    "            inds=list(range(self.output_size))\n",
    "            ind = np.random.choice(inds, p=y_pred.ravel())\n",
    "            s=s+td.i2c[ind]\n",
    "            X=np.array([[ind]])\n",
    "            Xt=torch.LongTensor(X)\n",
    "            ypl = self.forward(Xt,1)\n",
    "            ypl2 = ypl.view(-1,self.output_size)\n",
    "            if temperature>0.0:\n",
    "                ypl2 = ypl2 / temperature\n",
    "            yp = self.softmax(ypl2)\n",
    "        torch.set_grad_enabled(True)\n",
    "        return s    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymdU_wIWUlpK"
   },
   "source": [
    "## 2.3 Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH-FX-KylhEa"
   },
   "outputs": [],
   "source": [
    "td.init_tokenizer('char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pblj5uQLleqo"
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"model_name\": model_name,\n",
    "    \"vocab_size\": len(td.i2c),\n",
    "    \"neurons\": 256,\n",
    "    \"layers\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"steps\": 64,\n",
    "    \"batch_size\": 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwyg9BrLu3YE"
   },
   "outputs": [],
   "source": [
    "if ml_env.is_tpu:\n",
    "    # https://pytorch.org/xla/release/1.9/index.html\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device=xm.xla_device()  # untested!\n",
    "    logging.warning('Multi-core not yet implemented!')\n",
    "elif ml_env.is_gpu:\n",
    "    device='cuda'\n",
    "else:\n",
    "    device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7WuQ142UlpL"
   },
   "outputs": [],
   "source": [
    "poet = Poet(model_params['vocab_size'], model_params['neurons'], model_params['layers'], model_params['vocab_size'], device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJp6Sjw6UlpP"
   },
   "source": [
    "## 2.4 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeFxMxyuUlpQ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opti = torch.optim.Adam(poet.parameters(),lr=model_params['learning_rate']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtKdvxNymXpM"
   },
   "source": [
    "## 2.5 Helper Functions\n",
    "\n",
    "These allow to save or restore the training data. Saving and restoring can either be performed:\n",
    "\n",
    "* Jupyter: store/restore in a local directory,\n",
    "* Colab: store/restore on google drive. The training-code (using load_checkpoint()) will display an authentication url and code input-box in order to be able to access your google drive from this notebook. This allows to continue training sessions (or inference) after the Colab session was terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfmdQ6zCMy2L"
   },
   "outputs": [],
   "source": [
    "# snapshot_path=os.path.join(model_path, 'Snapshots')\n",
    "# os.makedirs(snapshot_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QebezmDgsJdh"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path,'model_params.json'),'w') as f:\n",
    "    json.dump(model_params,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3v82UHQsJdj"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, loss, pr, best_pr, filename='checkpoint.pth.tar'):\n",
    "    state={\n",
    "            'epoch': epoch,\n",
    "            'model_config': model_params,\n",
    "            'state_dict': poet.state_dict(),\n",
    "            'optimizer' : opti.state_dict(),\n",
    "            'precision': pr,\n",
    "            'loss': loss,\n",
    "        }\n",
    "    save_file=os.path.join(model_path, filename)\n",
    "    best_file=os.path.join(model_path,'model_best.pth.tar')\n",
    "    torch.save(state, save_file)\n",
    "    if pr >= best_pr:\n",
    "        shutil.copyfile(save_file, best_file )\n",
    "        print(f\"Saved best precision model, prec={pr:.3f}\")\n",
    "    else:\n",
    "        print(f\"Saved last model data, prec={pr:.3f}\")\n",
    "\n",
    "def save_history(history, filename=\"history.json\"):\n",
    "    save_file=os.path.join(model_path,filename)\n",
    "    try:\n",
    "        with open(save_file, 'w') as f:\n",
    "            json.dump(history, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write training history file {save_file}, {e}\")\n",
    "\n",
    "def load_history(filename=\"history.json\"):\n",
    "    load_file=os.path.join(model_path,filename)\n",
    "    try:\n",
    "        with open(load_file, 'r') as f:\n",
    "            history=json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Starting new history file {load_file}\")\n",
    "        return [], time.time()\n",
    "    if len(history)>0:\n",
    "        start=history[-1][\"timestamp\"]\n",
    "    return history, start\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    load_file=os.path.join(model_path,filename)\n",
    "    if not os.path.exists(load_file):\n",
    "        print(load_file)\n",
    "        print(\"No saved state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    state=torch.load(load_file)\n",
    "    mod_conf = state['model_config']\n",
    "    for param in ['model_name', 'learning_rate', 'batch_size']:\n",
    "        if mod_conf[param]!=model_params[param]:\n",
    "            print(f\"Warning: project {param} has changed from {mod_conf[param]} to {model_params[param]}\")\n",
    "            mod_conf[param]=model_params[param]\n",
    "    if model_params!=mod_conf:\n",
    "        print(f\"The saved model has an incompatible configuration than the current model: {mod_conf} vs. {model_params}\")\n",
    "        print(\"Cannot restore state, starting from scratch.\")\n",
    "        return 0,0\n",
    "    poet.load_state_dict(state['state_dict'])\n",
    "    opti.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    loss = state['loss']\n",
    "    best_pr = state['precision']\n",
    "    print(f\"Continuing from saved state epoch={epoch+1}, loss={loss:.3f}\")  # Save is not necessarily on epoch boundary, so that's approx.\n",
    "    return epoch,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_-ISha4nqhy"
   },
   "source": [
    "# 3. Training\n",
    "\n",
    "If there is already saved training data, this step is optional, and alternatively, ch. 4 can be continued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KDRpEm0n7B-"
   },
   "source": [
    "## 3.1 Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pZlsZ1Pnm5Z"
   },
   "outputs": [],
   "source": [
    "def torch_data_loader(batch_size, sample_length, device):\n",
    "    textlib_dataset=TextLibraryDataset(td, sample_length, device)\n",
    "    data_loader=torch.utils.data.DataLoader(textlib_dataset,batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return data_loader\n",
    "\n",
    "# Get one sample:\n",
    "# X, y = next(iter(data_loader))\n",
    "\n",
    "def precision(y, yp):\n",
    "    return (torch.sum(yp==y)/float((y.size()[0]*y.size()[1]))).item()\n",
    "\n",
    "def train(Xt, yt):\n",
    "    poet.zero_grad()\n",
    "\n",
    "    poet.init_hidden(Xt.size(0))\n",
    "    output = poet(Xt, model_params['steps'])\n",
    "    \n",
    "    olin=output.view(-1,model_params['vocab_size'])\n",
    "    _, ytp=torch.max(output,2)\n",
    "    ytlin=yt.view(-1)\n",
    "\n",
    "    pr=precision(yt,ytp)\n",
    "            \n",
    "    loss = criterion(olin, ytlin)\n",
    "    ls = loss.item()\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "\n",
    "    return ls, pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC36hNRKUlpU"
   },
   "source": [
    "## 3.2 The actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q9-3GUQ4UlpV",
    "outputId": "d7802f09-e7d2-409f-d5a2-3bbd81f381df",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False\n",
    "ls=0\n",
    "nr=0\n",
    "prs=0\n",
    "# torch.cuda.empty_cache()\n",
    "epoch_start, _ = load_checkpoint()\n",
    "history, start_time = load_history()\n",
    "pr=0.0\n",
    "best_pr=0.0\n",
    "\n",
    "data_loader=torch_data_loader(model_params['batch_size'], model_params['steps'], device)\n",
    "    \n",
    "# Make a snapshot of the trained parameters every snapshot_interval_sec\n",
    "snapshot_interval_sec=180\n",
    "# Generate text samples every sample_intervall_sec\n",
    "sample_interval_sec=600\n",
    "\n",
    "last_snapshot=time.time()\n",
    "last_sample=time.time()\n",
    "\n",
    "bench_all=0\n",
    "bench_data=0\n",
    "bench_train=0\n",
    "bench_sample=0\n",
    "bench_snapshot=0\n",
    "bench_output_times=3  # Give 3 benchmark outputs, then stop (it stays more or less same)\n",
    "sample_train_time=0\n",
    "\n",
    "for e in range(epoch_start,2500000):\n",
    "    t1=time.time()\n",
    "    t0=time.time()\n",
    "    for Xi,yi in data_loader:\n",
    "        t2=time.time()\n",
    "        # this cannot be done in data_loader, if multiprocessing is used :-/\n",
    "        Xt=Xi #.to(device)\n",
    "        yt=yi #.to(device)\n",
    "        \n",
    "        Xt.requires_grad_(False)\n",
    "        yt.requires_grad_(False)\n",
    "\n",
    "        bench_data += time.time()-t1\n",
    "        t1=time.time()\n",
    "        l, pr = train(Xt,yt)\n",
    "        bench_train += time.time()-t1\n",
    "\n",
    "        ls=ls+l\n",
    "        prs=prs+pr\n",
    "        nr=nr+1\n",
    "        cur_loss=ls/nr\n",
    "        cur_pr=prs/nr\n",
    "        if time.time()-last_snapshot > snapshot_interval_sec:\n",
    "            t1=time.time()\n",
    "            nr=0\n",
    "            ls=0\n",
    "            prs=0\n",
    "            if cur_pr>best_pr:\n",
    "                best_pr=cur_pr\n",
    "            last_snapshot=time.time()\n",
    "            print(f\"Epoch {e+1} Loss: {cur_loss:.3f} Precision: {cur_pr:.3f} Time/Sample: {sample_train_time:.6f} sec/sample\")\n",
    "            save_checkpoint(e,cur_loss,cur_pr, best_pr)\n",
    "            # if use_cuda:\n",
    "            #     print(f\"Cuda memory allocated: {torch.cuda.memory_allocated()} max_alloc: {torch.cuda.max_memory_allocated()} cached: {torch.cuda.memory_cached()} max_cached: {torch.cuda.max_memory_cached()}\")\n",
    "            hist={\"epoch\": e+1, \"loss\": cur_loss, \"precision\": cur_pr, \"timestamp\": time.time()-start_time}\n",
    "            history.append(hist)\n",
    "            save_history(history)\n",
    "            bench_snapshot+=time.time()-t1\n",
    "\n",
    "            if bench_all > 0 and bench_output_times>0:\n",
    "                bd=bench_data/bench_all*100.0\n",
    "                bt=bench_train/bench_all*100.0\n",
    "                bs=bench_sample/bench_all*100.0\n",
    "                bss=bench_snapshot/bench_all*100.0\n",
    "                bo=(bench_all-bench_data-bench_train-bench_sample-bench_snapshot)/bench_all*100.0\n",
    "                print(f\"Profiling: data-loading: {bd:.2f}%, training: {bt:.2f}%, sample gen: {bs:.2f}%, snapshots: {bss:.2f}%\", end=\"\")\n",
    "                bench_output_times = bench_output_times - 1\n",
    "                if bench_output_times == 0:\n",
    "                    print(f\" | Profiling finished.\")\n",
    "                else:\n",
    "                    print()\n",
    "\n",
    "                \n",
    "        sample_train_time=(time.time()-t2)/len(Xt)\n",
    "\n",
    "        if time.time()-last_sample > sample_interval_sec and cur_loss<1.5:\n",
    "            t1=time.time()\n",
    "            last_sample=time.time()\n",
    "            for temperature in [0.6, 0.7, 0.8]:\n",
    "                print(f\"Temperature {temperature}:\")\n",
    "                tgen=poet.generate(700,\". \", temperature=temperature)\n",
    "                td.source_highlight(tgen,min_quote_size=10,dark_mode=use_dark_mode,display_ref_anchor=False)\n",
    "            bench_sample+=time.time()-t1\n",
    "        t1=time.time()\n",
    "        bench_all+=time.time()-t0\n",
    "        t0=time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dsSPjPsUlpY"
   },
   "source": [
    "# 4. Text generation\n",
    "\n",
    "## 4.1 Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 59
    },
    "id": "u-vHDcHupruq",
    "outputId": "02335c56-f3b2-4641-bf2d-8ff26f94dadd"
   },
   "outputs": [],
   "source": [
    "load_checkpoint(filename=\"model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdpCtjvfUlpZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(generatedtext, textlibrary, min_quote_size=10, display_ref_anchor=True):\n",
    "    textlibrary.source_highlight(generatedtext, min_quote_size=min_quote_size, dark_mode=use_dark_mode, display_ref_anchor=display_ref_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oflaWxltsJd6",
    "outputId": "62bc62cf-d43d-4420-9dd9-7d412e6be818"
   },
   "outputs": [],
   "source": [
    "print(\"Sample text:\")\n",
    "print(\"\")\n",
    "for temperature in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    tgen=poet.generate(1000,\"\\n\\n\", temperature=temperature)\n",
    "    print(f\"================Temperature: {temperature}==============\")\n",
    "    detectPlagiarism(tgen, td, display_ref_anchor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjCd8CHLUlpd"
   },
   "source": [
    "## 4.2 Dialog with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfiL1_64Ulpe"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "def doDialog():\n",
    "    temperature = 0.6  # 0.1 (free-style chaos) - >1.0 (rigid, frozen)\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    numSentences = 3 # Try to generate numSentences terminated by endPrompt\n",
    "    # maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
    "    # maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    # minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    \n",
    "    print(\"Please enter some dialog.\")\n",
    "    print(\"The net will answer according to your input.\")\n",
    "    print(\"'bye' for end,\")\n",
    "    print(\"'reset' to reset the conversation context,\")\n",
    "    print(\"'temperature=<float>' [0.1(free, chaotic) - >1.0(strict, frozen)]\")\n",
    "    print(\"    to change character of the dialog.\")\n",
    "    # print(\"    Current temperature={}.\".format(temperature))\n",
    "    print()\n",
    "    xso = None\n",
    "    bye = False\n",
    "    last_ans=\"\"\n",
    "        \n",
    "    while not bye:\n",
    "        print(\"> \", end=\"\")\n",
    "        prompt = input()\n",
    "        if prompt == 'bye':\n",
    "            bye = True\n",
    "            print(\"Good bye!\")\n",
    "            continue\n",
    "        if prompt.find(\"temperature\")>=0 and prompt.find(\"=\") > prompt.find(\"temperature\"):\n",
    "            temperature=float(prompt[prompt.find('=')+1:])\n",
    "            print(f\"Temperature set to {temperature}\")\n",
    "            continue\n",
    "\n",
    "        prompt+=' '\n",
    "        for attempts in range(0,3):\n",
    "            tgen=poet.generate(2000,prompt,temperature=temperature)\n",
    "            # tgen=tgen.replace(\"Mr.\", \"Mr\")\n",
    "            # tgen=tgen.replace(\"Mrs.\", \"Mrs\")\n",
    "            # tgen=tgen.replace(\"\\n\",\" \")\n",
    "            # tgen=tgen.replace(\"  \",\" \")\n",
    "            tgi=tgen.split(\". \")\n",
    "            print(f\"{len(tgi)} sentences\")\n",
    "            if len(tgi)<numSentences:\n",
    "                continue\n",
    "            ans=\"\"\n",
    "            for i in range(0,numSentences):\n",
    "                ans += tgi[i]+\". \"\n",
    "            break\n",
    "            # i=tgen.find(endPrompt)\n",
    "            # i2=tgen[i+1:].find(endPrompt)+i\n",
    "            # i3=tgen[i2+1:].find(endPrompt)+i2\n",
    "            # i4=tgen[i3+1:].find(endPrompt)+i3\n",
    "            # tgen=tgen[i+1:i4+2]\n",
    "            # if len(tgen)>10:\n",
    "            #     break\n",
    "        last_ans=ans\n",
    "        td.source_highlight(last_ans, min_quote_size=10,dark_mode=use_dark_mode,display_ref_anchor=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8mGGsuFRUlpi",
    "outputId": "a2635d95-d05c-4865-ae93-9a29414ce1aa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eON9sYdz_1lh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "torch_poet_using_indie_tools.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
